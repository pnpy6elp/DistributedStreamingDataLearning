{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "fd6fb9fa",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import sys\n",
    "\n",
    "os.environ['PYSPARK_PYTHON'] = sys.executable\n",
    "os.environ['PYSPARK_DRIVER_PYTHON'] = sys.executable"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "8534a073",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using TensorFlow backend.\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "[('spark.eventLog.enabled', 'true'),\n",
       " ('spark.executor.instances', '3'),\n",
       " ('spark.sql.execution.arrow.enabled', 'true'),\n",
       " ('spark.driver.memory', '4g'),\n",
       " ('spark.executor.cores', '5'),\n",
       " ('spark.executor.extraJavaOptions', '-XX:+UseG1GC'),\n",
       " ('spark.driver.extraJavaOptions', '-XX:+UseG1GC'),\n",
       " ('spark.driver.host', 'kafka1'),\n",
       " ('spark.serializer', 'org.apache.spark.serializer.KryoSerializer'),\n",
       " ('spark.yarn.archive', 'hdfs:///user/spark/conf/spark-libs.jar'),\n",
       " ('spark.serializer.objectStreamReset', '100'),\n",
       " ('spark.sql.shuffle.partitions', '200'),\n",
       " ('spark.submit.deployMode', 'client'),\n",
       " ('spark.ui.filters',\n",
       "  'org.apache.hadoop.yarn.server.webproxy.amfilter.AmIpFilter'),\n",
       " ('spark.kryoserializer.buffer.max', '2047'),\n",
       " ('spark.executor.memoryOverhead', '1g'),\n",
       " ('spark.driver.memoryOverhead', '1g'),\n",
       " ('spark.executor.memory', '4g'),\n",
       " ('spark.executor.id', 'driver'),\n",
       " ('spark.history.fs.logDirectory', 'file:///root/spark/eventLog'),\n",
       " ('spark.default.parallelism', '163'),\n",
       " ('spark.app.name', 'PySparkShell'),\n",
       " ('spark.network.timeout', '3600s'),\n",
       " ('spark.driver.appUIAddress', 'http://kafka1:4040'),\n",
       " ('spark.executorEnv.PYTHONPATH',\n",
       "  '/root/spark/python/lib/py4j-0.10.7-src.zip:/root/spark/python/:/python:/python/lib/py4j-0.10.7-src.zip:<CPS>{{PWD}}/pyspark.zip<CPS>{{PWD}}/py4j-0.10.7-src.zip'),\n",
       " ('spark.org.apache.hadoop.yarn.server.webproxy.amfilter.AmIpFilter.param.PROXY_URI_BASES',\n",
       "  'http://0.0.0.0:8089/proxy/application_1665600846002_0001'),\n",
       " ('spark.master', 'yarn'),\n",
       " ('spark.ui.proxyBase', '/proxy/application_1665600846002_0001'),\n",
       " ('spark.app.id', 'application_1665600846002_0001'),\n",
       " ('spark.sql.catalogImplementation', 'hive'),\n",
       " ('spark.rdd.compress', 'True'),\n",
       " ('spark.org.apache.hadoop.yarn.server.webproxy.amfilter.AmIpFilter.param.PROXY_HOSTS',\n",
       "  '0.0.0.0'),\n",
       " ('spark.yarn.isPython', 'true'),\n",
       " ('spark.deploy.mode', 'client'),\n",
       " ('spark.eventLog.dir', 'file:///root/spark/eventLog'),\n",
       " ('spark.ui.showConsoleProgress', 'true'),\n",
       " ('spark.driver.port', '41937')]"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "!export PYSPARK_PYTHON=python3.6.9\n",
    "\n",
    "from tensorflow.keras.models import Sequential\n",
    "from tensorflow.keras.layers import Input, Dense, Conv2D\n",
    "from tensorflow.keras.layers import MaxPooling2D, Dropout,Flatten\n",
    "from tensorflow.keras import backend as K\n",
    "from tensorflow.keras.models import Model\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from tensorflow.keras import layers\n",
    "from tensorflow.keras.layers import Embedding\n",
    "import subprocess\n",
    "import tensorflow as tf\n",
    "from sklearn.feature_extraction import _stop_words\n",
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "from collections import Counter\n",
    "from nltk.corpus import brown\n",
    "from tensorflow.keras.layers.experimental.preprocessing import TextVectorization\n",
    "from pyspark import SparkContext\n",
    "from pyspark.sql.session import SparkSession\n",
    "from pyspark.streaming import StreamingContext\n",
    "from pyspark.ml import Pipeline\n",
    "from pyspark.ml.feature import StringIndexer, OneHotEncoderEstimator, VectorAssembler\n",
    "from pyspark.ml.feature import StopWordsRemover, Word2Vec, RegexTokenizer, Tokenizer\n",
    "from pyspark.ml.classification import LogisticRegression\n",
    "from pyspark.sql import Row\n",
    "from pyspark.streaming.kafka import KafkaUtils\n",
    "import pyspark.sql.functions as f\n",
    "import json\n",
    "import re\n",
    "from pyspark.sql import SparkSession, SQLContext\n",
    "from pyspark.sql.functions import *\n",
    "import sys\n",
    "from pyspark.sql.types import *\n",
    "from pyspark.sql.types import StructType, StructField, StringType, IntegerType\n",
    "from pyspark.sql.functions import from_json\n",
    "from pyspark.sql.functions import lit\n",
    "import pickle\n",
    "import tensorflow.keras\n",
    "from keras.preprocessing.text import Tokenizer\n",
    "from keras.preprocessing.sequence import pad_sequences\n",
    "from numpy import zeros\n",
    "from keras.models import model_from_json\n",
    "from tensorflow import keras\n",
    "from pyspark.sql.functions import udf\n",
    "import pandas as pd\n",
    "\n",
    "spark.sparkContext.getConf().getAll()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "9d57c5ec",
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark import SparkContext\n",
    "from pyspark.sql.session import SparkSession\n",
    "from pyspark.streaming import StreamingContext\n",
    "from pyspark.ml import Pipeline\n",
    "from pyspark.ml.feature import StringIndexer, OneHotEncoderEstimator, VectorAssembler\n",
    "from pyspark.ml.feature import StopWordsRemover, Word2Vec, RegexTokenizer, Tokenizer\n",
    "from pyspark.ml.classification import LogisticRegression\n",
    "from pyspark.sql import Row\n",
    "from pyspark.streaming.kafka import KafkaUtils\n",
    "import pyspark.sql.functions as f\n",
    "import json\n",
    "import re\n",
    "from pyspark.sql import SparkSession, SQLContext\n",
    "from pyspark.sql.functions import *\n",
    "import sys\n",
    "from pyspark.sql.types import *\n",
    "from pyspark.sql.types import StructType, StructField, StringType, IntegerType\n",
    "from pyspark.sql.functions import from_json\n",
    "from pyspark.sql.functions import lit\n",
    "\n",
    "import pickle\n",
    "import keras\n",
    "from keras.preprocessing.text import Tokenizer\n",
    "from keras.preprocessing.sequence import pad_sequences\n",
    "from numpy import zeros\n",
    "from keras.models import model_from_json"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "169bd619",
   "metadata": {},
   "outputs": [],
   "source": [
    "import keras\n",
    "from keras.models import Sequential\n",
    "from keras.layers import Input, Dense, Conv2D\n",
    "from keras.layers import MaxPooling2D, Dropout,Flatten\n",
    "from keras import backend as K\n",
    "from keras.models import Model\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from tensorflow.keras import layers\n",
    "from tensorflow.keras.layers import Embedding\n",
    "import subprocess\n",
    "import nltk"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "8d455813",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Error loading brown: <urlopen error [Errno -3] Temporary\n",
      "[nltk_data]     failure in name resolution>\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "False"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "nltk.download('brown')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "34172a44",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "\n",
       "            <div>\n",
       "                <p><b>SparkSession - hive</b></p>\n",
       "                \n",
       "        <div>\n",
       "            <p><b>SparkContext</b></p>\n",
       "\n",
       "            <p><a href=\"http://kafka1:4040\">Spark UI</a></p>\n",
       "\n",
       "            <dl>\n",
       "              <dt>Version</dt>\n",
       "                <dd><code>v2.4.7</code></dd>\n",
       "              <dt>Master</dt>\n",
       "                <dd><code>yarn</code></dd>\n",
       "              <dt>AppName</dt>\n",
       "                <dd><code>PySparkShell</code></dd>\n",
       "            </dl>\n",
       "        </div>\n",
       "        \n",
       "            </div>\n",
       "        "
      ],
      "text/plain": [
       "<pyspark.sql.session.SparkSession at 0x7f4f74987208>"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "spark"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "255dbbf0",
   "metadata": {},
   "outputs": [],
   "source": [
    "sc = spark._jsc.sc() "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "e21f8cf9",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "JavaObject id=o165"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sc"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "c8c60e1c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<py4j.java_gateway.JavaMember object at 0x7f4f749fa7b8>\n"
     ]
    }
   ],
   "source": [
    "print(sc.version)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "fc21f3f2",
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark import SparkContext\n",
    "from pyspark.sql.session import SparkSession\n",
    "from pyspark.streaming import StreamingContext\n",
    "from pyspark.ml import Pipeline\n",
    "from pyspark.ml.feature import StringIndexer, OneHotEncoderEstimator, VectorAssembler\n",
    "from pyspark.ml.feature import StopWordsRemover, Word2Vec, RegexTokenizer\n",
    "from pyspark.ml.classification import LogisticRegression\n",
    "from pyspark.sql import Row\n",
    "from pyspark.streaming.kafka import KafkaUtils\n",
    "import pyspark.sql.functions as f\n",
    "import json\n",
    "import re\n",
    "from pyspark.sql import SparkSession, SQLContext\n",
    "from pyspark.sql.functions import *\n",
    "import sys\n",
    "from pyspark.sql.types import *\n",
    "from pyspark.sql.types import StructType, StructField, StringType, IntegerType\n",
    "from pyspark.sql.functions import from_json\n",
    "from pyspark.sql.functions import lit\n",
    "\n",
    "import pickle\n",
    "import keras\n",
    "from keras.preprocessing.text import Tokenizer\n",
    "from keras.preprocessing.sequence import pad_sequences\n",
    "from numpy import zeros\n",
    "from keras.models import model_from_json"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "f609e123",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "\n",
       "            <div>\n",
       "                <p><b>SparkSession - hive</b></p>\n",
       "                \n",
       "        <div>\n",
       "            <p><b>SparkContext</b></p>\n",
       "\n",
       "            <p><a href=\"http://kafka1:4040\">Spark UI</a></p>\n",
       "\n",
       "            <dl>\n",
       "              <dt>Version</dt>\n",
       "                <dd><code>v2.4.7</code></dd>\n",
       "              <dt>Master</dt>\n",
       "                <dd><code>yarn</code></dd>\n",
       "              <dt>AppName</dt>\n",
       "                <dd><code>PySparkShell</code></dd>\n",
       "            </dl>\n",
       "        </div>\n",
       "        \n",
       "            </div>\n",
       "        "
      ],
      "text/plain": [
       "<pyspark.sql.session.SparkSession at 0x7f4f74987208>"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "spark "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "96b51caf",
   "metadata": {},
   "outputs": [],
   "source": [
    "sc = spark.sparkContext"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "7c620059",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "\n",
       "        <div>\n",
       "            <p><b>SparkContext</b></p>\n",
       "\n",
       "            <p><a href=\"http://kafka1:4040\">Spark UI</a></p>\n",
       "\n",
       "            <dl>\n",
       "              <dt>Version</dt>\n",
       "                <dd><code>v2.4.7</code></dd>\n",
       "              <dt>Master</dt>\n",
       "                <dd><code>yarn</code></dd>\n",
       "              <dt>AppName</dt>\n",
       "                <dd><code>PySparkShell</code></dd>\n",
       "            </dl>\n",
       "        </div>\n",
       "        "
      ],
      "text/plain": [
       "<SparkContext master=yarn appName=PySparkShell>"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sc"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "bc197658",
   "metadata": {},
   "outputs": [],
   "source": [
    "sql_context = SQLContext(sc)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "1454a8b0",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<pyspark.sql.context.SQLContext at 0x7f4efc8f9a58>"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sql_context"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "3d0795f8",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'yarn'"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sc.master"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e1fa041f",
   "metadata": {},
   "source": [
    "# Data Preprocessing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "ffb2ffab",
   "metadata": {},
   "outputs": [],
   "source": [
    "import re\n",
    "\n",
    "def regex_(text):\n",
    "    # 영어, 숫자, 특수만문자 제외 삭제.  \n",
    "    text = text.dropna(axis=0)\n",
    "    pattern = '(http|ftp|https)://(?:[-\\w.]|(?:%[\\da-fA-F]{2}))+/(?:[-\\w.]|(?:%[\\da-fA-F]{2}))+'\n",
    "    text['total_text_list'] = text['total_text_list'].str.replace(pat=pattern,repl=r'',regex=True)\n",
    "    pattern = '(http|ftp|https)://(?:[-\\w.]|(?:%[\\da-fA-F]{2}))+'  # URL제거\n",
    "    text['total_text_list'] = text['total_text_list'].str.replace(pat=pattern,repl=r'',regex=True)\n",
    "    pattern = '(http|ftp|https):// (?:[-\\w.]|(?:%[\\da-fA-F]{2}))+'  # URL제거\n",
    "    text['total_text_list'] = text['total_text_list'].str.replace(pat=pattern,repl=r'',regex=True)\n",
    "    text['total_text_list'] = text['total_text_list'].str.replace(pat=r'(\\@\\w+.*?) ',repl=r'',regex=True)\n",
    "    text['total_text_list'] = text['total_text_list'].str.replace(pat=r'[^ a-zA-Z]',repl=r'',regex=True)\n",
    "    text['total_text_list'] = text['total_text_list'].str.lower()\n",
    "    text=text[((text['total_text_list'].str.len()>= 10)) & ((text['total_text_list'].eq(' ')==False) & (text['total_text_list'].eq('')==False))]\n",
    "    return text\n",
    "def regex2_(text):\n",
    "    # 영어, 숫자, 특수만문자 제외 삭제.  \n",
    "    text = text.dropna(axis=0)\n",
    "    pattern = '(http|ftp|https)://(?:[-\\w.]|(?:%[\\da-fA-F]{2}))+/(?:[-\\w.]|(?:%[\\da-fA-F]{2}))+'\n",
    "    text['text'] = text['text'].str.replace(pat=pattern,repl=r'',regex=True)\n",
    "    pattern = '(http|ftp|https)://(?:[-\\w.]|(?:%[\\da-fA-F]{2}))+'  # URL제거\n",
    "    text['text'] = text['text'].str.replace(pat=pattern,repl=r'',regex=True)\n",
    "    pattern = '(http|ftp|https):// (?:[-\\w.]|(?:%[\\da-fA-F]{2}))+'  # URL제거\n",
    "    text['text'] = text['text'].str.replace(pat=pattern,repl=r'',regex=True)\n",
    "    text['text'] = text['text'].str.replace(pat=r'(\\@\\w+.*?) ',repl=r'',regex=True)\n",
    "    text['text'] = text['text'].str.replace(pat=r'[^ a-zA-Z]',repl=r'',regex=True)\n",
    "    text['text'] = text['text'].str.lower()\n",
    "    text=text[((text['text'].str.len()>= 10)) & ((text['text'].eq(' ')==False) & (text['text'].eq('')==False))]\n",
    "    return text\n",
    "\n",
    "def regex_3(text):\n",
    "    # 영어, 숫자, 특수만문자 제외 삭제.  \n",
    "    text = text.dropna(axis=0)\n",
    "    pattern = '(http|ftp|https)://(?:[-\\w.]|(?:%[\\da-fA-F]{2}))+/(?:[-\\w.]|(?:%[\\da-fA-F]{2}))+'\n",
    "    text['full_text'] = text['full_text'].str.replace(pat=pattern,repl=r'',regex=True)\n",
    "    pattern = '(http|ftp|https)://(?:[-\\w.]|(?:%[\\da-fA-F]{2}))+'  # URL제거\n",
    "    text['full_text'] = text['full_text'].str.replace(pat=pattern,repl=r'',regex=True)\n",
    "    pattern = '(http|ftp|https):// (?:[-\\w.]|(?:%[\\da-fA-F]{2}))+'  # URL제거\n",
    "    text['full_text'] = text['full_text'].str.replace(pat=pattern,repl=r'',regex=True)\n",
    "    text['full_text'] = text['full_text'].str.replace(pat=r'[^ a-zA-Z]',repl=r'',regex=True)\n",
    "    # @로 시작하는 애들도 정규식 이용해서 없애기 @[^ ]+\n",
    "    text['full_text'] = text['full_text'].str.replace(pat=r'(\\@\\w+.*?) ',repl=r'',regex=True)\n",
    "    text['full_text'] = text['full_text'].str.lower()\n",
    "    text=text[((text['full_text'].str.len()>= 10)) & ((text['full_text'].str.split(\" \").str.len()>= 10)) & ((text['full_text'].eq(' ')==False) & (text['full_text'].eq('')==False))]\n",
    "    return text\n",
    "\n",
    "def regex_4(text):\n",
    "    # 영어, 숫자, 특수만문자 제외 삭제.  \n",
    "    text = text.dropna(axis=0)\n",
    "    pattern = '(http|ftp|https)://(?:[-\\w.]|(?:%[\\da-fA-F]{2}))+/(?:[-\\w.]|(?:%[\\da-fA-F]{2}))+'\n",
    "    text['full_text'] = text['full_text'].str.replace(pat=pattern,repl=r'',regex=True)\n",
    "    pattern = '(http|ftp|https)://(?:[-\\w.]|(?:%[\\da-fA-F]{2}))+'  # URL제거\n",
    "    text['full_text'] = text['full_text'].str.replace(pat=pattern,repl=r'',regex=True)\n",
    "    pattern = '(http|ftp|https):// (?:[-\\w.]|(?:%[\\da-fA-F]{2}))+'  # URL제거\n",
    "    text['full_text'] = text['full_text'].str.replace(pat=pattern,repl=r'',regex=True)\n",
    "    text['full_text'] = text['full_text'].str.replace(pat=r'[^ a-zA-Z]',repl=r'',regex=True)\n",
    "    # @로 시작하는 애들도 정규식 이용해서 없애기 @[^ ]+\n",
    "    text['full_text'] = text['full_text'].str.replace(pat=r'(\\@\\w+.*?) ',repl=r'',regex=True)\n",
    "    text['full_text'] = text['full_text'].str.lower()\n",
    "    #text=text[((text['full_text'].str.len()>= 10)) & ((text['full_text'].str.split(\" \").str.len()>= 10)) & ((text['full_text'].eq(' ')==False) & (text['full_text'].eq('')==False))]\n",
    "    return text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "228b7b9a",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/root/spark/python/pyspark/sql/session.py:714: UserWarning: createDataFrame attempted Arrow optimization because 'spark.sql.execution.arrow.enabled' is set to true; however, failed by the reason below:\n",
      "  An error occurred while calling z:org.apache.spark.sql.api.python.PythonSQLUtils.readArrowStreamFromFile.\n",
      ": java.lang.IllegalArgumentException\n",
      "\tat java.nio.ByteBuffer.allocate(ByteBuffer.java:334)\n",
      "\tat org.apache.arrow.vector.ipc.message.MessageSerializer.readMessage(MessageSerializer.java:543)\n",
      "\tat org.apache.spark.sql.execution.arrow.ArrowConverters$$anon$3.readNextBatch(ArrowConverters.scala:243)\n",
      "\tat org.apache.spark.sql.execution.arrow.ArrowConverters$$anon$3.<init>(ArrowConverters.scala:229)\n",
      "\tat org.apache.spark.sql.execution.arrow.ArrowConverters$.getBatchesFromStream(ArrowConverters.scala:228)\n",
      "\tat org.apache.spark.sql.execution.arrow.ArrowConverters$$anonfun$readArrowStreamFromFile$2.apply(ArrowConverters.scala:216)\n",
      "\tat org.apache.spark.sql.execution.arrow.ArrowConverters$$anonfun$readArrowStreamFromFile$2.apply(ArrowConverters.scala:214)\n",
      "\tat org.apache.spark.util.Utils$.tryWithResource(Utils.scala:2543)\n",
      "\tat org.apache.spark.sql.execution.arrow.ArrowConverters$.readArrowStreamFromFile(ArrowConverters.scala:214)\n",
      "\tat org.apache.spark.sql.api.python.PythonSQLUtils$.readArrowStreamFromFile(PythonSQLUtils.scala:46)\n",
      "\tat org.apache.spark.sql.api.python.PythonSQLUtils.readArrowStreamFromFile(PythonSQLUtils.scala)\n",
      "\tat sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method)\n",
      "\tat sun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:62)\n",
      "\tat sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)\n",
      "\tat java.lang.reflect.Method.invoke(Method.java:498)\n",
      "\tat py4j.reflection.MethodInvoker.invoke(MethodInvoker.java:244)\n",
      "\tat py4j.reflection.ReflectionEngine.invoke(ReflectionEngine.java:357)\n",
      "\tat py4j.Gateway.invoke(Gateway.java:282)\n",
      "\tat py4j.commands.AbstractCommand.invokeMethod(AbstractCommand.java:132)\n",
      "\tat py4j.commands.CallCommand.execute(CallCommand.java:79)\n",
      "\tat py4j.GatewayConnection.run(GatewayConnection.java:238)\n",
      "\tat java.lang.Thread.run(Thread.java:748)\n",
      "\n",
      "Attempting non-optimization as 'spark.sql.execution.arrow.fallback.enabled' is set to true.\n",
      "  warnings.warn(msg)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+--------------------+-----+\n",
      "|                text|label|\n",
      "+--------------------+-----+\n",
      "|im at ap stgate i...|    0|\n",
      "|why do i love thi...|    0|\n",
      "|nowplaying charmi...|    0|\n",
      "|im at kuwait city...|    0|\n",
      "|lolim right hande...|    0|\n",
      "+--------------------+-----+\n",
      "only showing top 5 rows\n",
      "\n",
      "801932\n",
      "+--------------------+-----+\n",
      "|                text|label|\n",
      "+--------------------+-----+\n",
      "|preparing for tro...|    1|\n",
      "|weve got to prep ...|    1|\n",
      "|dont know how muc...|    1|\n",
      "|whoo hoo we are t...|    1|\n",
      "|hurricane kay is ...|    1|\n",
      "+--------------------+-----+\n",
      "only showing top 5 rows\n",
      "\n",
      "1640986\n"
     ]
    }
   ],
   "source": [
    "# general_minsun2.csv\n",
    "general = pd.read_csv('/root/spark/bigcomp_model/general.csv', sep=',',  lineterminator='\\n')\n",
    "general = general.drop_duplicates()\n",
    "general = spark.createDataFrame(general[['total_text_list']])\n",
    "general = general.select(col('total_text_list').alias('text')) \n",
    "general = general.withColumn('label', lit(0))\n",
    "general.show(5)\n",
    "print(general.count())\n",
    "general = general.withColumn(\"label\",col(\"label\").cast(\"integer\"))\n",
    "# 보안 관련 계정들 트윗\n",
    "security = pd.read_csv('/root/spark/bigcomp_model/typoon_final.csv', sep=',',  lineterminator='\\n')\n",
    "security\n",
    "security = spark.createDataFrame(security[['full_text']])\n",
    "security = security.select(col('full_text').alias('text')) \n",
    "security = security.withColumn('label', lit(1))\n",
    "security = security.withColumn(\"label\",col(\"label\").cast(\"integer\"))\n",
    "security.show(5)\n",
    "print(security.count())\n",
    " # 2007~2019\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "3e88c9df",
   "metadata": {},
   "outputs": [],
   "source": [
    "t05 = security.collect()[:500000]\n",
    "t9 = general.collect()[:500000]\n",
    "t05 = spark.createDataFrame(t05)\n",
    "t9 = spark.createDataFrame(t9)\n",
    "fifth = t05.union(t9)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "a9dcb2b4",
   "metadata": {},
   "outputs": [],
   "source": [
    "t1 = security.collect()[200000:250000]\n",
    "t2 = security.collect()[250000:300000]\n",
    "t3 = security.collect()[300000:350000]\n",
    "t4 = security.collect()[350000:400000]\n",
    "\n",
    "t5 = general.collect()[200000:250000]\n",
    "t6 = general.collect()[250000:300000]\n",
    "t7 = general.collect()[300000:350000]\n",
    "t8 = general.collect()[350000:400000]\n",
    "\n",
    "\n",
    "t1 = spark.createDataFrame(t1)\n",
    "t2 = spark.createDataFrame(t2)\n",
    "t3 = spark.createDataFrame(t3)\n",
    "t4 = spark.createDataFrame(t4)\n",
    "t5 = spark.createDataFrame(t5)\n",
    "t6 = spark.createDataFrame(t6)\n",
    "t7 = spark.createDataFrame(t7)\n",
    "t8 = spark.createDataFrame(t8)\n",
    "\n",
    "\n",
    "first = t1.union(t5)\n",
    "second = t1.union(t2).union(t5).union(t6)\n",
    "third = t1.union(t2).union(t3).union(t5).union(t6).union(t7)\n",
    "fourth = t1.union(t2).union(t3).union(t4).union(t5).union(t6).union(t7).union(t8)\n",
    "\n",
    "\n",
    "df1 = t1.union(t5)\n",
    "df2 = t2.union(t6)\n",
    "df3 = t3.union(t7)\n",
    "df4 = t4.union(t8)\n",
    "\n",
    "word1 = security.collect()[200000:250000]\n",
    "word2 = general.collect()[200000:250000]\n",
    "\n",
    "word3 = security.collect()[200000:300000]\n",
    "word4 = general.collect()[200000:300000]\n",
    "\n",
    "word5 = security.collect()[200000:350000]\n",
    "word6 = general.collect()[200000:350000]\n",
    "\n",
    "word7 = security.collect()[200000:400000]\n",
    "word8 = general.collect()[200000:400000]\n",
    "\n",
    "word1 = spark.createDataFrame(word1)\n",
    "word2= spark.createDataFrame(word2)\n",
    "\n",
    "word3 = spark.createDataFrame(word3)\n",
    "word4= spark.createDataFrame(word4)\n",
    "\n",
    "word5 = spark.createDataFrame(word5)\n",
    "word6= spark.createDataFrame(word6)\n",
    "\n",
    "word7 = spark.createDataFrame(word7)\n",
    "word8= spark.createDataFrame(word8)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "762700f3",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "100000\n",
      "105\n",
      "4221\n",
      "<class 'pyspark.sql.dataframe.DataFrame'>\n"
     ]
    }
   ],
   "source": [
    "from tensorflow.keras.preprocessing.text import Tokenizer\n",
    "def background_keyword(df): # input : spark dataframe을 pandas로 변환한 후, dataframe column\n",
    "    background = df.values.tolist()\n",
    "    token = Tokenizer()         # 토큰화 함수 지정\n",
    "    token.fit_on_texts(background)    # 토큰화 함수에 문장 적용\n",
    "    sorted_dic4 = sorted(token.word_counts.items(), key=lambda x: x[1], reverse=True)\n",
    "    word_counts_1 = {}\n",
    "    for i in sorted_dic4:\n",
    "        word_counts_1.update({i[0]:i[1]})\n",
    "    return word_counts_1 # dictionary 임\n",
    "def back_keyword(pos,neg): # pos : event-related keyword, neg : event-unrelated keyword\n",
    "    stage_1 = RegexTokenizer(inputCol= 'text', outputCol='pos_t', pattern= '\\\\W')\n",
    "    stage_2 = StopWordsRemover(inputCol= stage_1.getOutputCol(), outputCol= 'filtered_words')\n",
    "    word1 = stage_1.transform(pos) #나는 first_train 부터 함!\n",
    "    word1 = stage_2.transform(word1)\n",
    "    word2 = stage_1.transform(neg) #나는 first_train 부터 함!\n",
    "    word2 = stage_2.transform(word2)\n",
    "    from pyspark.sql.functions import col, concat_ws\n",
    "    df1 = word1.withColumn(\"filtered_words\",\n",
    "       concat_ws(\",\",col(\"filtered_words\")))\n",
    "    word1 = df1.select(\"filtered_words\").toPandas()\n",
    "    df2 = word2.withColumn(\"filtered_words\",\n",
    "       concat_ws(\",\",col(\"filtered_words\")))\n",
    "    word2 = df2.select(\"filtered_words\").toPandas()\n",
    "    word_counts_1 = background_keyword(word1[\"filtered_words\"])\n",
    "    word_counts_2 = background_keyword(word2[\"filtered_words\"])\n",
    "    return word_counts_1, word_counts_2\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "# Author : Minseon Kim (2021)\n",
    "class BroadcastWrapper(object):\n",
    "    def __init__(self, data, token_list):\n",
    "        self.broadcast_var = sc.broadcast(data)\n",
    "#         self.last_updated_time = datetime.now()\n",
    "        self.token_list = token_list\n",
    "    \n",
    "#     def is_should_be_updated(self, data):\n",
    "#         cur_time = datetime.now()\n",
    "#         diff_sec = (cur_time - self.last_updated_time).total_seconds()\n",
    "#         return self.broadcast_var is None or diff_sec> 1\n",
    "    \n",
    "    def update_and_get_data(self, spark):\n",
    "        a = self.broadcast_var.value\n",
    "        self.broadcast_var.unpersist()\n",
    "        for i in self.token_list:\n",
    "            for j in i:\n",
    "                if j not in a.keys():\n",
    "                    a[j] = 1\n",
    "                else:\n",
    "                    a[j] += 1\n",
    "        new_data = a\n",
    "        self.broadcast_var = spark.broadcast(new_data)\n",
    "#         self.last_updated_time = datetime.now()\n",
    "#         return len(self.token_list)\n",
    "        return self.broadcast_var\n",
    "\n",
    "# 바꾼 버전\n",
    "from pyspark.ml import Transformer\n",
    "from keras.preprocessing.text import Tokenizer\n",
    "from keras.preprocessing.sequence import pad_sequences\n",
    "import pandas as pd\n",
    "from typing import Dict\n",
    "from pyspark.sql import DataFrame\n",
    "from pyspark.sql import Row\n",
    "from pyspark.sql import types   \n",
    "\n",
    "def series_to_list(x):\n",
    "    seri = x.values.tolist()\n",
    "    for i in range(len(seri)):\n",
    "        seri[i] = seri[i].tolist()\n",
    "    return seri\n",
    "class TextToSequence(Transformer):\n",
    "    w1 = dict()\n",
    "    w2 = dict()\n",
    "    pos_t = Tokenizer(lower=False)\n",
    "    neg_t = Tokenizer(lower=False)\n",
    "    pos_vocab = []\n",
    "    neg_vocab = []\n",
    "    \n",
    "    def __init__(self, w1: Dict[str, int], w2:Dict[str, int]):\n",
    "        super(TextToSequence, self).__init__()\n",
    "        self.w1 = w1\n",
    "        self.w2 = w2\n",
    "        \n",
    "    \n",
    "    def _transform(self, df: DataFrame):\n",
    "        result_pdf = df.select(\"filtered_words\").toPandas()\n",
    "        pos_df = result_pdf[:150000] # 24939 99756\n",
    "        neg_df = result_pdf[150000:]\n",
    "        print(len(result_pdf))\n",
    "        \n",
    "        broadcast_wrapper1 = BroadcastWrapper(self.w1, pos_df[\"filtered_words\"])\n",
    "        ww1 = broadcast_wrapper1.update_and_get_data(sc).value\n",
    "        \n",
    "        broadcast_wrapper2 = BroadcastWrapper(self.w2, neg_df[\"filtered_words\"])\n",
    "        ww2 = broadcast_wrapper2.update_and_get_data(sc).value\n",
    "        \n",
    "        www1 = {k: v for k, v in sorted(ww1.items(), key=lambda item: item[1], reverse=True)}\n",
    "        www2 = {k: v for k, v in sorted(ww2.items(), key=lambda item: item[1], reverse=True)}\n",
    "        print(www1['allow'])\n",
    "        print(www2['like'])\n",
    "        aa = [w for i, w in enumerate(www1) if i < 5000]\n",
    "        bb = [w for i, w in enumerate(www2) if i < 5000]\n",
    "        \n",
    "        self.pos_t.fit_on_texts(aa)\n",
    "        self.neg_t.fit_on_texts(bb)\n",
    "#         encoded_docs_pos = self.pos_t.texts_to_sequences(df.select(\"filtered_words1\").toPandas())\n",
    "#         encoded_docs_neg = self.neg_t.texts_to_sequences(df.select(\"filtered_words1\").toPandas())\n",
    "        result_list = series_to_list(result_pdf[\"filtered_words\"])\n",
    "        encoded_docs_pos = self.pos_t.texts_to_sequences(result_list) # 상위 event keyword 5k가 쓰인 tokenizer\n",
    "        encoded_docs_neg = self.neg_t.texts_to_sequences(result_list)\n",
    "        \n",
    "        X_p = pad_sequences(encoded_docs_pos, maxlen=100, padding='post').tolist()\n",
    "        X_n = pad_sequences(encoded_docs_neg, maxlen=100, padding='post').tolist()\n",
    "        \n",
    "        text_array = [str(row['text']) for row in df.select('text').collect()]\n",
    "        label_array = [int(row['label']) for row in df.select('label').collect()]\n",
    "\n",
    "        \n",
    "        zip_array = list(zip(text_array, label_array, X_p, X_n))\n",
    "        rdd = sc.parallelize(zip_array, numSlices=306)\n",
    "        fdf = rdd.toDF(['text','label','feature1','feature2'])\n",
    "        \n",
    "\n",
    "        print(type(fdf))\n",
    "\n",
    "        return fdf, www1, aa\n",
    "    \n",
    "    \n",
    "word_counts_1, word_counts_2 = back_keyword(word1, word2)\n",
    "word_counts_1 = sc.broadcast(word_counts_1)\n",
    "word_counts_2 = sc.broadcast(word_counts_2)\n",
    "\n",
    "stage_1 = RegexTokenizer(inputCol= 'text', outputCol='pos_t', pattern= '\\\\W')\n",
    "stage_2 = StopWordsRemover(inputCol= stage_1.getOutputCol(), outputCol= 'filtered_words')\n",
    "\n",
    "r1 = stage_1.transform(df1) #나는 first_train 부터 함!\n",
    "r2 = stage_2.transform(r1)\n",
    "\n",
    "\n",
    "# USE THE TRANSFORMER WITHOUT PIPELINE\n",
    "text_sequence = TextToSequence(w1 = word_counts_1.value, w2 = word_counts_2.value)\n",
    "df_example, www2, bb = text_sequence.transform(r2)\n",
    "from pyspark.ml.linalg import Vectors, VectorUDT\n",
    "from pyspark.sql.functions import udf\n",
    "list_to_vector_udf = udf(lambda l: Vectors.dense(l), VectorUDT())\n",
    "df_with_vectors = df_example.select(\n",
    "    df_example[\"label\"], \n",
    "    list_to_vector_udf(df_example[\"feature1\"]).alias(\"feature1\"), \n",
    "    list_to_vector_udf(df_example[\"feature2\"]).alias(\"feature2\")\n",
    ")\n",
    "label_str_index = StringIndexer(inputCol='label', outputCol='label_index') # transformer의 마지막 단계!!\n",
    "label_df = label_str_index.fit(df_with_vectors).transform(df_with_vectors)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "9bffaa86",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1000000\n",
      "314\n",
      "81700\n",
      "<class 'pyspark.sql.dataframe.DataFrame'>\n"
     ]
    }
   ],
   "source": [
    "from tensorflow.keras.preprocessing.text import Tokenizer\n",
    "def background_keyword(df): # input : spark dataframe을 pandas로 변환한 후, dataframe column\n",
    "    background = df.values.tolist()\n",
    "    token = Tokenizer()         # 토큰화 함수 지정\n",
    "    token.fit_on_texts(background)    # 토큰화 함수에 문장 적용\n",
    "    sorted_dic4 = sorted(token.word_counts.items(), key=lambda x: x[1], reverse=True)\n",
    "    word_counts_1 = {}\n",
    "    for i in sorted_dic4:\n",
    "        word_counts_1.update({i[0]:i[1]})\n",
    "    return word_counts_1 # dictionary 임\n",
    "def back_keyword(pos,neg): # pos : event-related keyword, neg : event-unrelated keyword\n",
    "    stage_1 = RegexTokenizer(inputCol= 'text', outputCol='pos_t', pattern= '\\\\W')\n",
    "    stage_2 = StopWordsRemover(inputCol= stage_1.getOutputCol(), outputCol= 'filtered_words')\n",
    "    word1 = stage_1.transform(pos) #나는 first_train 부터 함!\n",
    "    word1 = stage_2.transform(word1)\n",
    "    word2 = stage_1.transform(neg) #나는 first_train 부터 함!\n",
    "    word2 = stage_2.transform(word2)\n",
    "    from pyspark.sql.functions import col, concat_ws\n",
    "    df1 = word1.withColumn(\"filtered_words\",\n",
    "       concat_ws(\",\",col(\"filtered_words\")))\n",
    "    word1 = df1.select(\"filtered_words\").toPandas()\n",
    "    df2 = word2.withColumn(\"filtered_words\",\n",
    "       concat_ws(\",\",col(\"filtered_words\")))\n",
    "    word2 = df2.select(\"filtered_words\").toPandas()\n",
    "    word_counts_1 = background_keyword(word1[\"filtered_words\"])\n",
    "    word_counts_2 = background_keyword(word2[\"filtered_words\"])\n",
    "    return word_counts_1, word_counts_2\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "# Author : Minseon Kim (2021)\n",
    "class BroadcastWrapper(object):\n",
    "    def __init__(self, data, token_list):\n",
    "        self.broadcast_var = sc.broadcast(data)\n",
    "#         self.last_updated_time = datetime.now()\n",
    "        self.token_list = token_list\n",
    "    \n",
    "#     def is_should_be_updated(self, data):\n",
    "#         cur_time = datetime.now()\n",
    "#         diff_sec = (cur_time - self.last_updated_time).total_seconds()\n",
    "#         return self.broadcast_var is None or diff_sec> 1\n",
    "    \n",
    "    def update_and_get_data(self, spark):\n",
    "        a = self.broadcast_var.value\n",
    "        self.broadcast_var.unpersist()\n",
    "        for i in self.token_list:\n",
    "            for j in i:\n",
    "                if j not in a.keys():\n",
    "                    a[j] = 1\n",
    "                else:\n",
    "                    a[j] += 1\n",
    "        new_data = a\n",
    "        self.broadcast_var = spark.broadcast(new_data)\n",
    "#         self.last_updated_time = datetime.now()\n",
    "#         return len(self.token_list)\n",
    "        return self.broadcast_var\n",
    "\n",
    "# 바꾼 버전\n",
    "from pyspark.ml import Transformer\n",
    "from keras.preprocessing.text import Tokenizer\n",
    "from keras.preprocessing.sequence import pad_sequences\n",
    "import pandas as pd\n",
    "from typing import Dict\n",
    "from pyspark.sql import DataFrame\n",
    "from pyspark.sql import Row\n",
    "from pyspark.sql import types   \n",
    "\n",
    "def series_to_list(x):\n",
    "    seri = x.values.tolist()\n",
    "    for i in range(len(seri)):\n",
    "        seri[i] = seri[i].tolist()\n",
    "    return seri\n",
    "class TextToSequence(Transformer):\n",
    "    w1 = dict()\n",
    "    w2 = dict()\n",
    "    pos_t = Tokenizer(lower=False)\n",
    "    neg_t = Tokenizer(lower=False)\n",
    "    pos_vocab = []\n",
    "    neg_vocab = []\n",
    "    \n",
    "    def __init__(self, w1: Dict[str, int], w2:Dict[str, int]):\n",
    "        super(TextToSequence, self).__init__()\n",
    "        self.w1 = w1\n",
    "        self.w2 = w2\n",
    "        \n",
    "    \n",
    "    def _transform(self, df: DataFrame):\n",
    "        result_pdf = df.select(\"filtered_words\").toPandas()\n",
    "        pos_df = result_pdf[:500000] # 24939 99756\n",
    "        neg_df = result_pdf[500000:]\n",
    "        print(len(result_pdf))\n",
    "        \n",
    "        broadcast_wrapper1 = BroadcastWrapper(self.w1, pos_df[\"filtered_words\"])\n",
    "        ww1 = broadcast_wrapper1.update_and_get_data(sc).value\n",
    "        \n",
    "        broadcast_wrapper2 = BroadcastWrapper(self.w2, neg_df[\"filtered_words\"])\n",
    "        ww2 = broadcast_wrapper2.update_and_get_data(sc).value\n",
    "        \n",
    "        www1 = {k: v for k, v in sorted(ww1.items(), key=lambda item: item[1], reverse=True)}\n",
    "        www2 = {k: v for k, v in sorted(ww2.items(), key=lambda item: item[1], reverse=True)}\n",
    "        print(www1['allow'])\n",
    "        print(www2['like'])\n",
    "        aa = [w for i, w in enumerate(www1) if i < 5000]\n",
    "        bb = [w for i, w in enumerate(www2) if i < 5000]\n",
    "        \n",
    "        self.pos_t.fit_on_texts(aa)\n",
    "        self.neg_t.fit_on_texts(bb)\n",
    "#         encoded_docs_pos = self.pos_t.texts_to_sequences(df.select(\"filtered_words1\").toPandas())\n",
    "#         encoded_docs_neg = self.neg_t.texts_to_sequences(df.select(\"filtered_words1\").toPandas())\n",
    "        result_list = series_to_list(result_pdf[\"filtered_words\"])\n",
    "        encoded_docs_pos = self.pos_t.texts_to_sequences(result_list) # 상위 event keyword 5k가 쓰인 tokenizer\n",
    "        encoded_docs_neg = self.neg_t.texts_to_sequences(result_list)\n",
    "        \n",
    "        X_p = pad_sequences(encoded_docs_pos, maxlen=100, padding='post').tolist()\n",
    "        X_n = pad_sequences(encoded_docs_neg, maxlen=100, padding='post').tolist()\n",
    "        \n",
    "        text_array = [str(row['text']) for row in df.select('text').collect()]\n",
    "        label_array = [int(row['label']) for row in df.select('label').collect()]\n",
    "\n",
    "        \n",
    "        zip_array = list(zip(text_array, label_array, X_p, X_n))\n",
    "        rdd = sc.parallelize(zip_array, numSlices=306)\n",
    "        fdf = rdd.toDF(['text','label','feature1','feature2'])\n",
    "        \n",
    "\n",
    "        print(type(fdf))\n",
    "\n",
    "        return fdf, www1, aa\n",
    "    \n",
    "    \n",
    "word_counts_1, word_counts_2 = back_keyword(t05, t9)\n",
    "word_counts_1 = sc.broadcast(word_counts_1)\n",
    "word_counts_2 = sc.broadcast(word_counts_2)\n",
    "\n",
    "stage_1 = RegexTokenizer(inputCol= 'text', outputCol='pos_t', pattern= '\\\\W')\n",
    "stage_2 = StopWordsRemover(inputCol= stage_1.getOutputCol(), outputCol= 'filtered_words')\n",
    "\n",
    "r1 = stage_1.transform(fifth) #나는 first_train 부터 함!\n",
    "r2 = stage_2.transform(r1)\n",
    "\n",
    "\n",
    "# USE THE TRANSFORMER WITHOUT PIPELINE\n",
    "text_sequence = TextToSequence(w1 = word_counts_1.value, w2 = word_counts_2.value)\n",
    "df_example, www2, bb = text_sequence.transform(r2)\n",
    "from pyspark.ml.linalg import Vectors, VectorUDT\n",
    "from pyspark.sql.functions import udf\n",
    "list_to_vector_udf = udf(lambda l: Vectors.dense(l), VectorUDT())\n",
    "df_with_vectors = df_example.select(\n",
    "    df_example[\"label\"], \n",
    "    list_to_vector_udf(df_example[\"feature1\"]).alias(\"feature1\"), \n",
    "    list_to_vector_udf(df_example[\"feature2\"]).alias(\"feature2\")\n",
    ")\n",
    "label_str_index = StringIndexer(inputCol='label', outputCol='label_index') # transformer의 마지막 단계!!\n",
    "label_df2 = label_str_index.fit(df_with_vectors).transform(df_with_vectors)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "09df52f1",
   "metadata": {},
   "outputs": [],
   "source": [
    "trainqq, testqq = label_df.randomSplit(weights=[0.8,0.2]) # first train"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "fa5bff5a",
   "metadata": {},
   "outputs": [],
   "source": [
    "trainqq1, testqq1 = label_df2.randomSplit(weights=[0.8,0.2]) # fifth window (no finetuning)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "1a87b3ca",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/root/spark/python/pyspark/sql/dataframe.py:2111: UserWarning: toPandas attempted Arrow optimization because 'spark.sql.execution.arrow.enabled' is set to true; however, failed by the reason below:\n",
      "  Unsupported type in conversion to Arrow: VectorUDT\n",
      "Attempting non-optimization as 'spark.sql.execution.arrow.fallback.enabled' is set to true.\n",
      "  warnings.warn(msg)\n"
     ]
    }
   ],
   "source": [
    "trainqq.toPandas().to_csv(\"/root/spark/bigcomp_model/typoon/first_train.csv\",index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "9744e0ee",
   "metadata": {},
   "outputs": [],
   "source": [
    "testqq.toPandas().to_csv(\"/root/spark/bigcomp_model/typoon/first_test.csv\",index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "ec0f8bf1",
   "metadata": {},
   "outputs": [],
   "source": [
    "trainqq1.toPandas().to_csv(\"/root/spark/bigcomp_model/typoon/fifth_tr2.csv\",index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "1446f1dd",
   "metadata": {},
   "outputs": [],
   "source": [
    "testqq1.toPandas().to_csv(\"/root/spark/bigcomp_model/typoon/fifth_ts2.csv\",index=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6d3b8cf6",
   "metadata": {},
   "source": [
    "# 데이터프레임"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "8ca73bcb",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/root/spark/python/pyspark/sql/session.py:714: UserWarning: createDataFrame attempted Arrow optimization because 'spark.sql.execution.arrow.enabled' is set to true; however, failed by the reason below:\n",
      "  An error occurred while calling z:org.apache.spark.sql.api.python.PythonSQLUtils.readArrowStreamFromFile.\n",
      ": java.lang.IllegalArgumentException\n",
      "\tat java.nio.ByteBuffer.allocate(ByteBuffer.java:334)\n",
      "\tat org.apache.arrow.vector.ipc.message.MessageSerializer.readMessage(MessageSerializer.java:543)\n",
      "\tat org.apache.spark.sql.execution.arrow.ArrowConverters$$anon$3.readNextBatch(ArrowConverters.scala:243)\n",
      "\tat org.apache.spark.sql.execution.arrow.ArrowConverters$$anon$3.<init>(ArrowConverters.scala:229)\n",
      "\tat org.apache.spark.sql.execution.arrow.ArrowConverters$.getBatchesFromStream(ArrowConverters.scala:228)\n",
      "\tat org.apache.spark.sql.execution.arrow.ArrowConverters$$anonfun$readArrowStreamFromFile$2.apply(ArrowConverters.scala:216)\n",
      "\tat org.apache.spark.sql.execution.arrow.ArrowConverters$$anonfun$readArrowStreamFromFile$2.apply(ArrowConverters.scala:214)\n",
      "\tat org.apache.spark.util.Utils$.tryWithResource(Utils.scala:2543)\n",
      "\tat org.apache.spark.sql.execution.arrow.ArrowConverters$.readArrowStreamFromFile(ArrowConverters.scala:214)\n",
      "\tat org.apache.spark.sql.api.python.PythonSQLUtils$.readArrowStreamFromFile(PythonSQLUtils.scala:46)\n",
      "\tat org.apache.spark.sql.api.python.PythonSQLUtils.readArrowStreamFromFile(PythonSQLUtils.scala)\n",
      "\tat sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method)\n",
      "\tat sun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:62)\n",
      "\tat sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)\n",
      "\tat java.lang.reflect.Method.invoke(Method.java:498)\n",
      "\tat py4j.reflection.MethodInvoker.invoke(MethodInvoker.java:244)\n",
      "\tat py4j.reflection.ReflectionEngine.invoke(ReflectionEngine.java:357)\n",
      "\tat py4j.Gateway.invoke(Gateway.java:282)\n",
      "\tat py4j.commands.AbstractCommand.invokeMethod(AbstractCommand.java:132)\n",
      "\tat py4j.commands.CallCommand.execute(CallCommand.java:79)\n",
      "\tat py4j.GatewayConnection.run(GatewayConnection.java:238)\n",
      "\tat java.lang.Thread.run(Thread.java:748)\n",
      "\n",
      "Attempting non-optimization as 'spark.sql.execution.arrow.fallback.enabled' is set to true.\n",
      "  warnings.warn(msg)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-----+--------------------+--------------------+-----------+\n",
      "|label|            feature1|            feature2|label_index|\n",
      "+-----+--------------------+--------------------+-----------+\n",
      "|    1|[1.0,8.0,71.0,1.0...|[288.0,552.0,3346...|        1.0|\n",
      "|    1|[1.0,8.0,129.0,10...|[288.0,692.0,122....|        1.0|\n",
      "|    1|[1.0,42.0,742.0,1...|[3779.0,3148.0,1....|        1.0|\n",
      "|    1|[1.0,44.0,12.0,87...|[5.0,4857.0,89.0,...|        1.0|\n",
      "|    1|[1.0,44.0,12.0,87...|[5.0,4857.0,89.0,...|        1.0|\n",
      "+-----+--------------------+--------------------+-----------+\n",
      "only showing top 5 rows\n",
      "\n",
      "+-----+--------------------+--------------------+-----------+\n",
      "|label|            feature1|            feature2|label_index|\n",
      "+-----+--------------------+--------------------+-----------+\n",
      "|    1|[1.0,8.0,66.0,69....|[288.0,138.0,64.0...|        1.0|\n",
      "|    1|[1.0,8.0,1902.0,1...|[288.0,2569.0,555...|        1.0|\n",
      "|    1|[1.0,44.0,574.0,1...|[544.0,4993.0,796...|        1.0|\n",
      "|    1|[1.0,44.0,1534.0,...|[138.0,64.0,311.0...|        1.0|\n",
      "|    1|[1.0,51.0,69.0,28...|[4518.0,64.0,311....|        1.0|\n",
      "+-----+--------------------+--------------------+-----------+\n",
      "only showing top 5 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "trainqq= pd.read_csv(\"/root/spark/bigcomp_model/typoon/first_train.csv\", sep=',',  lineterminator='\\n')\n",
    "trainqq = spark.createDataFrame(trainqq[['label','feature1','feature2','label_index']])\n",
    "trainqq.show(5)\n",
    "testqq= pd.read_csv(\"/root/spark/bigcomp_model/typoon/first_test.csv\", sep=',',  lineterminator='\\n')\n",
    "testqq = spark.createDataFrame(testqq[['label','feature1','feature2','label_index']])\n",
    "testqq.show(5)\n",
    "trainqq1 = pd.read_csv(\"/root/spark/bigcomp_model/typoon/fifth_tr2.csv\", sep=',',  lineterminator='\\n')\n",
    "trainqq1 = spark.createDataFrame(trainqq1[['label','feature1','feature2','label_index']])\n",
    "testqq1 = pd.read_csv(\"/root/spark/bigcomp_model/typoon/fifth_ts2.csv\", sep=',',  lineterminator='\\n')\n",
    "testqq1 = spark.createDataFrame(testqq1[['label','feature1','feature2','label_index']])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "3c93dc31",
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.ml.linalg import Vectors, VectorUDT\n",
    "from pyspark.sql.functions import udf\n",
    "import pyspark.sql.types as T\n",
    "\n",
    "parse = udf(lambda l: Vectors.dense(l), VectorUDT())\n",
    "\n",
    "import pyspark.sql.functions as F\n",
    "import pyspark.sql.types as T\n",
    "import json\n",
    "\n",
    "def parse_embedding_from_string(x):\n",
    "    res = json.loads(x)\n",
    "    return res\n",
    "\n",
    "retrieve_embedding = F.udf(parse_embedding_from_string, T.ArrayType(T.LongType()))\n",
    "\n",
    "trainqq = trainqq.select(\n",
    "    trainqq[\"label_index\"], \n",
    "    retrieve_embedding(trainqq[\"feature1\"]).alias(\"feature1\"),\n",
    "    retrieve_embedding(trainqq[\"feature2\"]).alias(\"feature2\"),\n",
    "    trainqq[\"label\"]\n",
    ")\n",
    "trainqq = trainqq.select(\n",
    "    trainqq[\"label_index\"], \n",
    "    parse(trainqq[\"feature1\"]).alias(\"feature1\"),\n",
    "    parse(trainqq[\"feature2\"]).alias(\"feature2\"),\n",
    "    trainqq[\"label\"]\n",
    ")\n",
    "\n",
    "\n",
    "\n",
    "trainqq1 = trainqq1.select(\n",
    "    trainqq1[\"label_index\"], \n",
    "    retrieve_embedding(trainqq1[\"feature1\"]).alias(\"feature1\"),\n",
    "    retrieve_embedding(trainqq1[\"feature2\"]).alias(\"feature2\"),\n",
    "    trainqq1[\"label\"]\n",
    ")\n",
    "trainqq1 = trainqq1.select(\n",
    "    trainqq1[\"label_index\"], \n",
    "    parse(trainqq1[\"feature1\"]).alias(\"feature1\"),\n",
    "    parse(trainqq1[\"feature2\"]).alias(\"feature2\"),\n",
    "    trainqq1[\"label\"]\n",
    ")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "e4f43a1c",
   "metadata": {},
   "outputs": [],
   "source": [
    "testqq = testqq.select(\n",
    "    testqq[\"label_index\"], \n",
    "    retrieve_embedding(testqq[\"feature1\"]).alias(\"feature1\"),\n",
    "    retrieve_embedding(testqq[\"feature2\"]).alias(\"feature2\"),\n",
    "    testqq[\"label\"]\n",
    ")\n",
    "testqq = testqq.select(\n",
    "    testqq[\"label_index\"], \n",
    "    parse(testqq[\"feature1\"]).alias(\"feature1\"),\n",
    "    parse(testqq[\"feature2\"]).alias(\"feature2\"),\n",
    "    testqq[\"label\"]\n",
    ")\n",
    "testqq1 = testqq1.select(\n",
    "    testqq1[\"label_index\"], \n",
    "    retrieve_embedding(testqq1[\"feature1\"]).alias(\"feature1\"),\n",
    "    retrieve_embedding(testqq1[\"feature2\"]).alias(\"feature2\"),\n",
    "    testqq1[\"label\"]\n",
    ")\n",
    "testqq1 = testqq1.select(\n",
    "    testqq1[\"label_index\"], \n",
    "    parse(testqq1[\"feature1\"]).alias(\"feature1\"),\n",
    "    parse(testqq1[\"feature2\"]).alias(\"feature2\"),\n",
    "    testqq1[\"label\"]\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "15a0428f",
   "metadata": {},
   "source": [
    "# Classifier"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "3cb90fb9",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"sequential\"\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "embedding (Embedding)        (None, 100, 100)          500100    \n",
      "_________________________________________________________________\n",
      "dropout (Dropout)            (None, 100, 100)          0         \n",
      "_________________________________________________________________\n",
      "conv1d (Conv1D)              (None, 96, 256)           128256    \n",
      "_________________________________________________________________\n",
      "global_max_pooling1d (Global (None, 256)               0         \n",
      "_________________________________________________________________\n",
      "dense (Dense)                (None, 250)               64250     \n",
      "_________________________________________________________________\n",
      "dropout_1 (Dropout)          (None, 250)               0         \n",
      "_________________________________________________________________\n",
      "dense_1 (Dense)              (None, 2)                 502       \n",
      "_________________________________________________________________\n",
      "activation (Activation)      (None, 2)                 0         \n",
      "=================================================================\n",
      "Total params: 693,108\n",
      "Trainable params: 693,108\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n",
      "None\n"
     ]
    }
   ],
   "source": [
    "from tensorflow.python.keras.models import Model, Sequential\n",
    "from tensorflow.python.keras.layers import Input, Dense, concatenate\n",
    "from tensorflow.python.keras.layers import LSTM\n",
    "from tensorflow.python.keras.layers.embeddings import Embedding\n",
    "from tensorflow.python.keras.layers import Conv1D, Flatten, GlobalMaxPooling1D, Dropout, MaxPooling1D, Activation, Bidirectional, Conv2D, GlobalMaxPooling2D\n",
    "nb_classes=2\n",
    "vocab_sizes = 5001\n",
    "neg_vocab_sizes = 5001\n",
    "embedding_vector_length = 100\n",
    "hidden_dims = 250\n",
    "max_length = 100\n",
    "dropout_ratio = 0.5\n",
    "from tensorflow.keras.callbacks import ModelCheckpoint\n",
    "\n",
    "model = Sequential()\n",
    "model.add(Embedding(vocab_sizes, embedding_vector_length, input_length=max_length))\n",
    "\n",
    "#어휘 크기(최대 정수 인덱스+1, 고밀도 임베딩의 치수, 입력 시퀀스의 길이\n",
    "\n",
    "# 모델은 크기의 정수 행렬 (배치,\n",
    "# input_length) 및 입력에서 가장 큰 정수 (즉, 단어 인덱스)\n",
    "#은 999 (어휘 크기)보다 크지 않아야합니다.\n",
    "# 이제 model.output_shape는 (None, 10, 64)이고, 여기서`None`은 배치입니다.\n",
    "# dimension.\n",
    "model.add(Dropout(dropout_ratio))\n",
    "model.add(Conv1D(256, 5, activation='relu'))\n",
    "#model.add(Bidirectional(LSTM(128)))\n",
    "model.add(GlobalMaxPooling1D())\n",
    "model.add(Dense(250, activation='relu')) # 한 층 더 쌓을까..? relu 함수는 정류된 함수로 은닉층에서 많이 사용된다. -를 차단함!\n",
    "# 은닉층으로 많이 사용되는 이유는 음수는 0으로 반환하므로 특정 양수값으로 수렴하지 않음. 그래서 기울기 소실이 발생하지 않음!\n",
    "# 시그모이드 함수는 기울기 소실이 발생한다는 단점이 존재. 그래서 relu를 은닉층으로 쓰는 거임!\n",
    "# 또한, 학습 속도가 매우 빠르다.\n",
    "model.add(Dropout(dropout_ratio))\n",
    "model.add(Dense(nb_classes)) # stringIndexer 클래스 수만큼 쌓는 이유가 뭘까,,,\n",
    "model.add(Activation('sigmoid'))\n",
    "#model.add(Dense(1, activation='sigmoid'))\n",
    "model.compile(loss='binary_crossentropy', optimizer='adam', metrics=['accuracy'])\n",
    "\"\"\"#model.add(Dropout(dropout_ratio))\n",
    "model.add(Conv1D(256, 3,  padding='valid', activation='relu'))\n",
    "model.add(GlobalMaxPooling1D())\n",
    "#model.add(Dense(128, activation='relu')) #\n",
    "model.add(Dropout(dropout_ratio))\n",
    "model.add(Activation('relu'))\n",
    "model.add(Dense(nb_classes)) # stringIndexer 클래스 수만큼 쌓는 이유가 뭘까,,,\n",
    "model.add(Activation('sigmoid'))\n",
    "#model.add(Dense(1, activation='sigmoid'))\n",
    "model.compile(loss='binary_crossentropy', optimizer='adam', metrics=['accuracy'])\"\"\"\n",
    "\n",
    "\"\"\"model.add(Bidirectional(LSTM(128)))\n",
    "# model.add(GlobalMaxPooling1D())\n",
    "model.add(Activation('relu')) # 한 층 더 쌓을까..? relu 함수는 정류된 함수로 은닉층에서 많이 사용된다. -를 차단함!\n",
    "# 은닉층으로 많이 사용되는 이유는 음수는 0으로 반환하므로 특정 양수값으로 수렴하지 않음. 그래서 기울기 소실이 발생하지 않음!\n",
    "# 시그모이드 함수는 기울기 소실이 발생한다는 단점이 존재. 그래서 relu를 은닉층으로 쓰는 거임!\n",
    "# 또한, 학습 속도가 매우 빠르다.\"\"\"\n",
    "\"\"\"model.add(Dense(nb_classes)) # stringIndexer 클래스 수만큼 쌓는 이유가 뭘까,,,\n",
    "model.add(Activation('sigmoid'))\n",
    "# model.add(Dense(1, activation='sigmoid'))\n",
    "model.compile(loss='binary_crossentropy', optimizer='adam', metrics=['accuracy'])\"\"\"\n",
    "\n",
    "# #체크포인트 생성?!\n",
    "# # checkpoint_filepath = '/root/spark/baseline/'\n",
    "# fname = \"checkpoint-{epoch:02d}-{val_loss:.2f}\"\n",
    "# checkpoint = ModelCheckpoint(fname, monitor=\"val_loss\", mode=\"min\",\n",
    "#     save_best_only=True, verbose=1)\n",
    "# callbacks = [checkpoint]\n",
    "\n",
    "print(model.summary())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "id": "2f118c09",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"sequential_1\"\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "embedding_1 (Embedding)      (None, 100, 100)          500100    \n",
      "_________________________________________________________________\n",
      "bidirectional_1 (Bidirection (None, 256)               234496    \n",
      "_________________________________________________________________\n",
      "activation_2 (Activation)    (None, 256)               0         \n",
      "_________________________________________________________________\n",
      "dense_1 (Dense)              (None, 2)                 514       \n",
      "_________________________________________________________________\n",
      "activation_3 (Activation)    (None, 2)                 0         \n",
      "=================================================================\n",
      "Total params: 735,110\n",
      "Trainable params: 735,110\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n",
      "None\n"
     ]
    }
   ],
   "source": [
    "from tensorflow.python.keras.models import Model, Sequential\n",
    "from tensorflow.python.keras.layers import Input, Dense, concatenate\n",
    "from tensorflow.python.keras.layers import LSTM\n",
    "from tensorflow.python.keras.layers.embeddings import Embedding\n",
    "from tensorflow.python.keras.layers import Conv1D, Flatten, GlobalMaxPooling1D, Dropout, MaxPooling1D, Activation, Bidirectional, Conv2D, GlobalMaxPooling2D\n",
    "nb_classes=2\n",
    "vocab_sizes = 5001\n",
    "neg_vocab_sizes = 5001\n",
    "embedding_vector_length = 100\n",
    "hidden_dims = 250\n",
    "max_length = 100\n",
    "dropout_ratio = 0.5\n",
    "from tensorflow.keras.callbacks import ModelCheckpoint\n",
    "\n",
    "model = Sequential()\n",
    "model.add(Embedding(vocab_sizes, embedding_vector_length, input_length=max_length))\n",
    "\n",
    "#어휘 크기(최대 정수 인덱스+1, 고밀도 임베딩의 치수, 입력 시퀀스의 길이\n",
    "\n",
    "# 모델은 크기의 정수 행렬 (배치,\n",
    "# input_length) 및 입력에서 가장 큰 정수 (즉, 단어 인덱스)\n",
    "#은 999 (어휘 크기)보다 크지 않아야합니다.\n",
    "# 이제 model.output_shape는 (None, 10, 64)이고, 여기서`None`은 배치입니다.\n",
    "# dimension.\n",
    "\"\"\"model.add(Dropout(dropout_ratio))\n",
    "model.add(Conv1D(256, 5, activation='relu'))\n",
    "#model.add(Bidirectional(LSTM(128)))\n",
    "model.add(GlobalMaxPooling1D())\n",
    "model.add(Dense(250, activation='relu')) # 한 층 더 쌓을까..? relu 함수는 정류된 함수로 은닉층에서 많이 사용된다. -를 차단함!\n",
    "# 은닉층으로 많이 사용되는 이유는 음수는 0으로 반환하므로 특정 양수값으로 수렴하지 않음. 그래서 기울기 소실이 발생하지 않음!\n",
    "# 시그모이드 함수는 기울기 소실이 발생한다는 단점이 존재. 그래서 relu를 은닉층으로 쓰는 거임!\n",
    "# 또한, 학습 속도가 매우 빠르다.\n",
    "model.add(Dropout(dropout_ratio))\n",
    "model.add(Dense(nb_classes)) # stringIndexer 클래스 수만큼 쌓는 이유가 뭘까,,,\n",
    "model.add(Activation('sigmoid'))\n",
    "#model.add(Dense(1, activation='sigmoid'))\n",
    "model.compile(loss='binary_crossentropy', optimizer='adam', metrics=['accuracy'])\"\"\"\n",
    "\"\"\"#model.add(Dropout(dropout_ratio))\n",
    "model.add(Conv1D(256, 3,  padding='valid', activation='relu'))\n",
    "model.add(GlobalMaxPooling1D())\n",
    "#model.add(Dense(128, activation='relu')) #\n",
    "model.add(Dropout(dropout_ratio))\n",
    "model.add(Activation('relu'))\n",
    "model.add(Dense(nb_classes)) # stringIndexer 클래스 수만큼 쌓는 이유가 뭘까,,,\n",
    "model.add(Activation('sigmoid'))\n",
    "#model.add(Dense(1, activation='sigmoid'))\n",
    "model.compile(loss='binary_crossentropy', optimizer='adam', metrics=['accuracy'])\"\"\"\n",
    "\n",
    "model.add(Bidirectional(LSTM(128)))\n",
    "# model.add(GlobalMaxPooling1D())\n",
    "model.add(Activation('relu')) # 한 층 더 쌓을까..? relu 함수는 정류된 함수로 은닉층에서 많이 사용된다. -를 차단함!\n",
    "# 은닉층으로 많이 사용되는 이유는 음수는 0으로 반환하므로 특정 양수값으로 수렴하지 않음. 그래서 기울기 소실이 발생하지 않음!\n",
    "# 시그모이드 함수는 기울기 소실이 발생한다는 단점이 존재. 그래서 relu를 은닉층으로 쓰는 거임!\n",
    "# 또한, 학습 속도가 매우 빠르다.\n",
    "model.add(Dense(nb_classes)) # stringIndexer 클래스 수만큼 쌓는 이유가 뭘까,,,\n",
    "model.add(Activation('sigmoid'))\n",
    "# model.add(Dense(1, activation='sigmoid'))\n",
    "model.compile(loss='binary_crossentropy', optimizer='adam', metrics=['accuracy'])\n",
    "\n",
    "# #체크포인트 생성?!\n",
    "# # checkpoint_filepath = '/root/spark/baseline/'\n",
    "# fname = \"checkpoint-{epoch:02d}-{val_loss:.2f}\"\n",
    "# checkpoint = ModelCheckpoint(fname, monitor=\"val_loss\", mode=\"min\",\n",
    "#     save_best_only=True, verbose=1)\n",
    "# callbacks = [checkpoint]\n",
    "\n",
    "print(model.summary())"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4d0d3401",
   "metadata": {},
   "source": [
    "# fine tuning"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "7db82d6f",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"sequential_2\"\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "embedding_2 (Embedding)      (None, 100, 100)          500100    \n",
      "_________________________________________________________________\n",
      "dropout (Dropout)            (None, 100, 100)          0         \n",
      "_________________________________________________________________\n",
      "conv1d (Conv1D)              (None, 98, 256)           77056     \n",
      "_________________________________________________________________\n",
      "global_max_pooling1d (Global (None, 256)               0         \n",
      "_________________________________________________________________\n",
      "dense_2 (Dense)              (None, 128)               32896     \n",
      "_________________________________________________________________\n",
      "dropout_1 (Dropout)          (None, 128)               0         \n",
      "_________________________________________________________________\n",
      "dense_3 (Dense)              (None, 1)                 129       \n",
      "=================================================================\n",
      "Total params: 610,181\n",
      "Trainable params: 610,181\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n",
      "None\n",
      "Model: \"sequential\"\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "embedding_2 (Embedding)      (None, 100, 100)          500100    \n",
      "_________________________________________________________________\n",
      "dropout (Dropout)            (None, 100, 100)          0         \n",
      "_________________________________________________________________\n",
      "conv1d (Conv1D)              (None, 98, 256)           77056     \n",
      "_________________________________________________________________\n",
      "global_max_pooling1d (Global (None, 256)               0         \n",
      "_________________________________________________________________\n",
      "dense_2 (Dense)              (None, 128)               32896     \n",
      "_________________________________________________________________\n",
      "dropout_1 (Dropout)          (None, 128)               0         \n",
      "_________________________________________________________________\n",
      "dense (Dense)                (None, 2)                 258       \n",
      "_________________________________________________________________\n",
      "activation (Activation)      (None, 2)                 0         \n",
      "=================================================================\n",
      "Total params: 610,310\n",
      "Trainable params: 258\n",
      "Non-trainable params: 610,052\n",
      "_________________________________________________________________\n",
      "None\n"
     ]
    }
   ],
   "source": [
    "from tensorflow.keras.models import load_model\n",
    "from tensorflow.python.keras.models import Model, Sequential\n",
    "from tensorflow.python.keras.layers import Input, Dense, concatenate\n",
    "from tensorflow.python.keras.layers import LSTM\n",
    "from tensorflow.python.keras.layers.embeddings import Embedding\n",
    "from tensorflow.python.keras.layers import Conv1D, Flatten, GlobalMaxPooling1D, Dropout, MaxPooling1D, Activation, Bidirectional, Conv2D, GlobalMaxPooling2D\n",
    "model1 = load_model('./bigcomp_model/model/typretrained_cnn.h5')\n",
    "print(model1.summary())\n",
    "model1.layers[0].trainable = False\n",
    "model1.layers[1].trainable = False\n",
    "model1.layers[2].trainable = False\n",
    "model1.layers[3].trainable = False\n",
    "model1.layers[4].trainable = False\n",
    "model1.layers[5].trainable = False\n",
    "trans_model = Sequential(model1.layers[:6])\n",
    "trans_model.add(Dense(2))\n",
    "trans_model.add(Activation('sigmoid'))\n",
    "trans_model.compile(loss='binary_crossentropy', optimizer='adam', metrics=['accuracy'])\n",
    "\n",
    "print(trans_model.summary())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "13334541",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"sequential\"\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "embedding (Embedding)        (None, 100, 100)          500100    \n",
      "_________________________________________________________________\n",
      "simple_rnn (SimpleRNN)       (None, 32)                4256      \n",
      "_________________________________________________________________\n",
      "dense (Dense)                (None, 1)                 33        \n",
      "=================================================================\n",
      "Total params: 504,389\n",
      "Trainable params: 504,389\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n",
      "None\n",
      "Model: \"sequential_1\"\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "embedding (Embedding)        (None, 100, 100)          500100    \n",
      "_________________________________________________________________\n",
      "simple_rnn (SimpleRNN)       (None, 32)                4256      \n",
      "_________________________________________________________________\n",
      "dense_1 (Dense)              (None, 2)                 66        \n",
      "_________________________________________________________________\n",
      "activation_1 (Activation)    (None, 2)                 0         \n",
      "=================================================================\n",
      "Total params: 504,422\n",
      "Trainable params: 66\n",
      "Non-trainable params: 504,356\n",
      "_________________________________________________________________\n",
      "None\n"
     ]
    }
   ],
   "source": [
    "from tensorflow.keras.models import load_model\n",
    "from tensorflow.python.keras.models import Model, Sequential\n",
    "from tensorflow.python.keras.layers import Input, Dense, concatenate\n",
    "from tensorflow.python.keras.layers import LSTM\n",
    "from tensorflow.python.keras.layers.embeddings import Embedding\n",
    "from tensorflow.python.keras.layers import Conv1D, Flatten, GlobalMaxPooling1D, Dropout, MaxPooling1D, Activation, Bidirectional, Conv2D, GlobalMaxPooling2D\n",
    "model1 = load_model('./bigcomp_model/model/typretrained_rnn.h5')\n",
    "print(model1.summary())\n",
    "model1.layers[0].trainable = False\n",
    "model1.layers[1].trainable = False\n",
    "trans_model = Sequential(model1.layers[:2])\n",
    "trans_model.add(Dense(2))\n",
    "trans_model.add(Activation('sigmoid'))\n",
    "trans_model.compile(loss='binary_crossentropy', optimizer='adam', metrics=['accuracy'])\n",
    "\n",
    "print(trans_model.summary())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "id": "f5366d7d",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[<tensorflow.python.keras.layers.embeddings.Embedding at 0x7f3313b971d0>,\n",
       " <tensorflow.python.keras.layers.recurrent.SimpleRNN at 0x7f3313b864a8>,\n",
       " <tensorflow.python.keras.layers.core.Dense at 0x7f3395418160>]"
      ]
     },
     "execution_count": 68,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model1.layers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0054899b",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0c8335e4",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "b2a7fcab",
   "metadata": {},
   "outputs": [],
   "source": [
    "from keras import optimizers, regularizers\n",
    "from tensorflow.keras import optimizers\n",
    "\n",
    "optimizer_conf = optimizers.Adam(lr=0.01)\n",
    "opt_conf = optimizers.serialize(optimizer_conf)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "dfcf276e",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "ElephasEstimator_9759056e21ee"
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from tensorflow.keras.models import model_from_json\n",
    "from elephas import spark_model, ml_model\n",
    "from elephas.ml_model import ElephasEstimator\n",
    "\n",
    "estimator = ElephasEstimator()\n",
    "estimator.setFeaturesCol(\"feature1\")\n",
    "estimator.setLabelCol(\"label_index\")\n",
    "#estimator.set_keras_model_config(trans_model.to_json())\n",
    "estimator.set_keras_model_config(model.to_json())\n",
    "estimator.set_categorical_labels(True) # dense 1이면 False로 설정해야함\n",
    "estimator.set_nb_classes(2)\n",
    "estimator.set_num_workers(3)\n",
    "estimator.set_epochs(10)\n",
    "estimator.set_batch_size(128)\n",
    "estimator.set_verbosity(1)\n",
    "estimator.set_validation_split(0.10)\n",
    "estimator.set_optimizer_config(opt_conf)\n",
    "estimator.set_mode(\"asynchronous\")\n",
    "estimator.set_loss(\"binary_crossentropy\")\n",
    "estimator.set_metrics(['acc'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "4ba54659",
   "metadata": {},
   "outputs": [],
   "source": [
    "dl_pipeline = Pipeline(stages=[estimator])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "5a6c3b94",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-----------+--------------------+--------------------+-----+\n",
      "|label_index|            feature1|            feature2|label|\n",
      "+-----------+--------------------+--------------------+-----+\n",
      "|        1.0|[1.0,8.0,71.0,1.0...|[288.0,552.0,3346...|    1|\n",
      "|        1.0|[1.0,8.0,129.0,10...|[288.0,692.0,122....|    1|\n",
      "|        1.0|[1.0,42.0,742.0,1...|[3779.0,3148.0,1....|    1|\n",
      "|        1.0|[1.0,44.0,12.0,87...|[5.0,4857.0,89.0,...|    1|\n",
      "|        1.0|[1.0,44.0,12.0,87...|[5.0,4857.0,89.0,...|    1|\n",
      "|        1.0|[1.0,44.0,31.0,20...|[16.0,81.0,755.0,...|    1|\n",
      "|        1.0|[1.0,44.0,66.0,69...|[138.0,64.0,3075....|    1|\n",
      "|        1.0|[1.0,44.0,93.0,15...|[108.0,1672.0,311...|    1|\n",
      "|        1.0|[1.0,44.0,399.0,7...|[2641.0,79.0,4345...|    1|\n",
      "|        1.0|[1.0,44.0,570.0,1...|[710.0,113.0,0.0,...|    1|\n",
      "+-----------+--------------------+--------------------+-----+\n",
      "only showing top 10 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "trainqq.show(10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "10f4da92",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      ">>> Fit model\n",
      " * Serving Flask app 'elephas.parameter.server' (lazy loading)\n",
      " * Environment: production\n",
      "\u001b[31m   WARNING: This is a development server. Do not use it in a production deployment.\u001b[0m\n",
      "\u001b[2m   Use a production WSGI server instead.\u001b[0m\n",
      " * Debug mode: off\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " * Running on http://172.28.0.1:4000/ (Press CTRL+C to quit)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      ">>> Initialize workers\n",
      ">>> Distribute load\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "172.28.0.1 - - [12/Oct/2022 18:58:29] \"GET /parameters HTTP/1.1\" 200 -\n",
      "172.28.0.1 - - [12/Oct/2022 18:58:29] \"GET /parameters HTTP/1.1\" 200 -\n",
      "172.28.0.1 - - [12/Oct/2022 18:58:29] \"GET /parameters HTTP/1.1\" 200 -\n",
      "172.28.0.1 - - [12/Oct/2022 18:58:50] \"POST /update HTTP/1.1\" 200 -\n",
      "172.28.0.1 - - [12/Oct/2022 18:58:50] \"GET /parameters HTTP/1.1\" 200 -\n",
      "172.28.0.1 - - [12/Oct/2022 18:58:50] \"POST /update HTTP/1.1\" 200 -\n",
      "172.28.0.1 - - [12/Oct/2022 18:58:50] \"GET /parameters HTTP/1.1\" 200 -\n",
      "172.28.0.1 - - [12/Oct/2022 18:58:51] \"POST /update HTTP/1.1\" 200 -\n",
      "172.28.0.1 - - [12/Oct/2022 18:58:51] \"GET /parameters HTTP/1.1\" 200 -\n",
      "172.28.0.1 - - [12/Oct/2022 18:59:11] \"POST /update HTTP/1.1\" 200 -\n",
      "172.28.0.1 - - [12/Oct/2022 18:59:11] \"GET /parameters HTTP/1.1\" 200 -\n",
      "172.28.0.1 - - [12/Oct/2022 18:59:11] \"POST /update HTTP/1.1\" 200 -\n",
      "172.28.0.1 - - [12/Oct/2022 18:59:11] \"GET /parameters HTTP/1.1\" 200 -\n",
      "172.28.0.1 - - [12/Oct/2022 18:59:12] \"POST /update HTTP/1.1\" 200 -\n",
      "172.28.0.1 - - [12/Oct/2022 18:59:12] \"GET /parameters HTTP/1.1\" 200 -\n",
      "172.28.0.1 - - [12/Oct/2022 18:59:31] \"POST /update HTTP/1.1\" 200 -\n",
      "172.28.0.1 - - [12/Oct/2022 18:59:31] \"GET /parameters HTTP/1.1\" 200 -\n",
      "172.28.0.1 - - [12/Oct/2022 18:59:33] \"POST /update HTTP/1.1\" 200 -\n",
      "172.28.0.1 - - [12/Oct/2022 18:59:33] \"GET /parameters HTTP/1.1\" 200 -\n",
      "172.28.0.1 - - [12/Oct/2022 18:59:33] \"POST /update HTTP/1.1\" 200 -\n",
      "172.28.0.1 - - [12/Oct/2022 18:59:33] \"GET /parameters HTTP/1.1\" 200 -\n",
      "172.28.0.1 - - [12/Oct/2022 18:59:52] \"POST /update HTTP/1.1\" 200 -\n",
      "172.28.0.1 - - [12/Oct/2022 18:59:52] \"GET /parameters HTTP/1.1\" 200 -\n",
      "172.28.0.1 - - [12/Oct/2022 18:59:54] \"POST /update HTTP/1.1\" 200 -\n",
      "172.28.0.1 - - [12/Oct/2022 18:59:54] \"GET /parameters HTTP/1.1\" 200 -\n",
      "172.28.0.1 - - [12/Oct/2022 18:59:54] \"POST /update HTTP/1.1\" 200 -\n",
      "172.28.0.1 - - [12/Oct/2022 18:59:54] \"GET /parameters HTTP/1.1\" 200 -\n",
      "172.28.0.1 - - [12/Oct/2022 19:00:13] \"POST /update HTTP/1.1\" 200 -\n",
      "172.28.0.1 - - [12/Oct/2022 19:00:13] \"GET /parameters HTTP/1.1\" 200 -\n",
      "172.28.0.1 - - [12/Oct/2022 19:00:15] \"POST /update HTTP/1.1\" 200 -\n",
      "172.28.0.1 - - [12/Oct/2022 19:00:15] \"GET /parameters HTTP/1.1\" 200 -\n",
      "172.28.0.1 - - [12/Oct/2022 19:00:15] \"POST /update HTTP/1.1\" 200 -\n",
      "172.28.0.1 - - [12/Oct/2022 19:00:15] \"GET /parameters HTTP/1.1\" 200 -\n",
      "172.28.0.1 - - [12/Oct/2022 19:00:33] \"POST /update HTTP/1.1\" 200 -\n",
      "172.28.0.1 - - [12/Oct/2022 19:00:33] \"GET /parameters HTTP/1.1\" 200 -\n",
      "172.28.0.1 - - [12/Oct/2022 19:00:36] \"POST /update HTTP/1.1\" 200 -\n",
      "172.28.0.1 - - [12/Oct/2022 19:00:36] \"POST /update HTTP/1.1\" 200 -\n",
      "172.28.0.1 - - [12/Oct/2022 19:00:36] \"GET /parameters HTTP/1.1\" 200 -\n",
      "172.28.0.1 - - [12/Oct/2022 19:00:36] \"GET /parameters HTTP/1.1\" 200 -\n",
      "172.28.0.1 - - [12/Oct/2022 19:00:54] \"POST /update HTTP/1.1\" 200 -\n",
      "172.28.0.1 - - [12/Oct/2022 19:00:54] \"GET /parameters HTTP/1.1\" 200 -\n",
      "172.28.0.1 - - [12/Oct/2022 19:00:58] \"POST /update HTTP/1.1\" 200 -\n",
      "172.28.0.1 - - [12/Oct/2022 19:00:58] \"GET /parameters HTTP/1.1\" 200 -\n",
      "172.28.0.1 - - [12/Oct/2022 19:00:58] \"POST /update HTTP/1.1\" 200 -\n",
      "172.28.0.1 - - [12/Oct/2022 19:00:58] \"GET /parameters HTTP/1.1\" 200 -\n",
      "172.28.0.1 - - [12/Oct/2022 19:01:14] \"POST /update HTTP/1.1\" 200 -\n",
      "172.28.0.1 - - [12/Oct/2022 19:01:14] \"GET /parameters HTTP/1.1\" 200 -\n",
      "172.28.0.1 - - [12/Oct/2022 19:01:19] \"POST /update HTTP/1.1\" 200 -\n",
      "172.28.0.1 - - [12/Oct/2022 19:01:19] \"GET /parameters HTTP/1.1\" 200 -\n",
      "172.28.0.1 - - [12/Oct/2022 19:01:19] \"POST /update HTTP/1.1\" 200 -\n",
      "172.28.0.1 - - [12/Oct/2022 19:01:19] \"GET /parameters HTTP/1.1\" 200 -\n",
      "172.28.0.1 - - [12/Oct/2022 19:01:35] \"POST /update HTTP/1.1\" 200 -\n",
      "172.28.0.1 - - [12/Oct/2022 19:01:35] \"GET /parameters HTTP/1.1\" 200 -\n",
      "172.28.0.1 - - [12/Oct/2022 19:01:40] \"POST /update HTTP/1.1\" 200 -\n",
      "172.28.0.1 - - [12/Oct/2022 19:01:40] \"GET /parameters HTTP/1.1\" 200 -\n",
      "172.28.0.1 - - [12/Oct/2022 19:01:40] \"POST /update HTTP/1.1\" 200 -\n",
      "172.28.0.1 - - [12/Oct/2022 19:01:40] \"GET /parameters HTTP/1.1\" 200 -\n",
      "172.28.0.1 - - [12/Oct/2022 19:01:55] \"POST /update HTTP/1.1\" 200 -\n",
      "172.28.0.1 - - [12/Oct/2022 19:01:59] \"POST /update HTTP/1.1\" 200 -\n",
      "172.28.0.1 - - [12/Oct/2022 19:01:59] \"POST /update HTTP/1.1\" 200 -\n",
      "172.28.0.1 - - [12/Oct/2022 19:01:59] \"GET /parameters HTTP/1.1\" 200 -\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      ">>> Async training complete.\n",
      "255.6982741355896\n"
     ]
    }
   ],
   "source": [
    "import time\n",
    "t1 = time.time()\n",
    "my_dl = dl_pipeline.fit(trainqq) # cnn first train (not finetuning)\n",
    "t3 = time.time() - t1\n",
    "print(t3)\n",
    "aaaaaa = \"완료!!\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "6549712c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ElephasTransformer_557762362fc8\n"
     ]
    }
   ],
   "source": [
    "cs_model = my_dl.stages[0]\n",
    "print(cs_model)\n",
    "\n",
    "cs_model.save(\"/root/spark/bigcomp_model/model/typoon/cnn/first2\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "79a4efc7",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bc5040b0",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "cacef6b1",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      ">>> Fit model\n",
      " * Serving Flask app 'elephas.parameter.server' (lazy loading)\n",
      " * Environment: production\n",
      "\u001b[31m   WARNING: This is a development server. Do not use it in a production deployment.\u001b[0m\n",
      "\u001b[2m   Use a production WSGI server instead.\u001b[0m\n",
      " * Debug mode: off\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " * Running on http://172.28.0.1:4000/ (Press CTRL+C to quit)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      ">>> Initialize workers\n",
      ">>> Distribute load\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "172.28.0.1 - - [12/Oct/2022 19:06:11] \"GET /parameters HTTP/1.1\" 200 -\n",
      "172.28.0.1 - - [12/Oct/2022 19:07:03] \"GET /parameters HTTP/1.1\" 200 -\n",
      "172.28.0.1 - - [12/Oct/2022 19:07:06] \"GET /parameters HTTP/1.1\" 200 -\n",
      "172.28.0.1 - - [12/Oct/2022 19:10:13] \"POST /update HTTP/1.1\" 200 -\n",
      "172.28.0.1 - - [12/Oct/2022 19:10:13] \"GET /parameters HTTP/1.1\" 200 -\n",
      "172.28.0.1 - - [12/Oct/2022 19:10:41] \"POST /update HTTP/1.1\" 200 -\n",
      "172.28.0.1 - - [12/Oct/2022 19:10:41] \"GET /parameters HTTP/1.1\" 200 -\n",
      "172.28.0.1 - - [12/Oct/2022 19:10:41] \"POST /update HTTP/1.1\" 200 -\n",
      "172.28.0.1 - - [12/Oct/2022 19:10:41] \"GET /parameters HTTP/1.1\" 200 -\n",
      "172.28.0.1 - - [12/Oct/2022 19:13:45] \"POST /update HTTP/1.1\" 200 -\n",
      "172.28.0.1 - - [12/Oct/2022 19:13:45] \"GET /parameters HTTP/1.1\" 200 -\n",
      "172.28.0.1 - - [12/Oct/2022 19:14:13] \"POST /update HTTP/1.1\" 200 -\n",
      "172.28.0.1 - - [12/Oct/2022 19:14:13] \"GET /parameters HTTP/1.1\" 200 -\n",
      "172.28.0.1 - - [12/Oct/2022 19:14:14] \"POST /update HTTP/1.1\" 200 -\n",
      "172.28.0.1 - - [12/Oct/2022 19:14:15] \"GET /parameters HTTP/1.1\" 200 -\n",
      "172.28.0.1 - - [12/Oct/2022 19:17:17] \"POST /update HTTP/1.1\" 200 -\n",
      "172.28.0.1 - - [12/Oct/2022 19:17:17] \"GET /parameters HTTP/1.1\" 200 -\n",
      "172.28.0.1 - - [12/Oct/2022 19:17:44] \"POST /update HTTP/1.1\" 200 -\n",
      "172.28.0.1 - - [12/Oct/2022 19:17:44] \"GET /parameters HTTP/1.1\" 200 -\n",
      "172.28.0.1 - - [12/Oct/2022 19:17:47] \"POST /update HTTP/1.1\" 200 -\n",
      "172.28.0.1 - - [12/Oct/2022 19:17:47] \"GET /parameters HTTP/1.1\" 200 -\n",
      "172.28.0.1 - - [12/Oct/2022 19:20:48] \"POST /update HTTP/1.1\" 200 -\n",
      "172.28.0.1 - - [12/Oct/2022 19:20:48] \"GET /parameters HTTP/1.1\" 200 -\n",
      "172.28.0.1 - - [12/Oct/2022 19:21:15] \"POST /update HTTP/1.1\" 200 -\n",
      "172.28.0.1 - - [12/Oct/2022 19:21:15] \"GET /parameters HTTP/1.1\" 200 -\n",
      "172.28.0.1 - - [12/Oct/2022 19:21:19] \"POST /update HTTP/1.1\" 200 -\n",
      "172.28.0.1 - - [12/Oct/2022 19:21:19] \"GET /parameters HTTP/1.1\" 200 -\n",
      "172.28.0.1 - - [12/Oct/2022 19:24:20] \"POST /update HTTP/1.1\" 200 -\n",
      "172.28.0.1 - - [12/Oct/2022 19:24:20] \"GET /parameters HTTP/1.1\" 200 -\n",
      "172.28.0.1 - - [12/Oct/2022 19:24:46] \"POST /update HTTP/1.1\" 200 -\n",
      "172.28.0.1 - - [12/Oct/2022 19:24:46] \"GET /parameters HTTP/1.1\" 200 -\n",
      "172.28.0.1 - - [12/Oct/2022 19:24:51] \"POST /update HTTP/1.1\" 200 -\n",
      "172.28.0.1 - - [12/Oct/2022 19:24:51] \"GET /parameters HTTP/1.1\" 200 -\n",
      "172.28.0.1 - - [12/Oct/2022 19:27:51] \"POST /update HTTP/1.1\" 200 -\n",
      "172.28.0.1 - - [12/Oct/2022 19:27:51] \"GET /parameters HTTP/1.1\" 200 -\n",
      "172.28.0.1 - - [12/Oct/2022 19:28:17] \"POST /update HTTP/1.1\" 200 -\n",
      "172.28.0.1 - - [12/Oct/2022 19:28:17] \"GET /parameters HTTP/1.1\" 200 -\n",
      "172.28.0.1 - - [12/Oct/2022 19:28:24] \"POST /update HTTP/1.1\" 200 -\n",
      "172.28.0.1 - - [12/Oct/2022 19:28:24] \"GET /parameters HTTP/1.1\" 200 -\n",
      "172.28.0.1 - - [12/Oct/2022 19:31:22] \"POST /update HTTP/1.1\" 200 -\n",
      "172.28.0.1 - - [12/Oct/2022 19:31:22] \"GET /parameters HTTP/1.1\" 200 -\n",
      "172.28.0.1 - - [12/Oct/2022 19:31:48] \"POST /update HTTP/1.1\" 200 -\n",
      "172.28.0.1 - - [12/Oct/2022 19:31:48] \"GET /parameters HTTP/1.1\" 200 -\n",
      "172.28.0.1 - - [12/Oct/2022 19:31:55] \"POST /update HTTP/1.1\" 200 -\n",
      "172.28.0.1 - - [12/Oct/2022 19:31:55] \"GET /parameters HTTP/1.1\" 200 -\n",
      "172.28.0.1 - - [12/Oct/2022 19:34:53] \"POST /update HTTP/1.1\" 200 -\n",
      "172.28.0.1 - - [12/Oct/2022 19:34:53] \"GET /parameters HTTP/1.1\" 200 -\n",
      "172.28.0.1 - - [12/Oct/2022 19:35:19] \"POST /update HTTP/1.1\" 200 -\n",
      "172.28.0.1 - - [12/Oct/2022 19:35:19] \"GET /parameters HTTP/1.1\" 200 -\n",
      "172.28.0.1 - - [12/Oct/2022 19:35:27] \"POST /update HTTP/1.1\" 200 -\n",
      "172.28.0.1 - - [12/Oct/2022 19:35:27] \"GET /parameters HTTP/1.1\" 200 -\n",
      "172.28.0.1 - - [12/Oct/2022 19:38:24] \"POST /update HTTP/1.1\" 200 -\n",
      "172.28.0.1 - - [12/Oct/2022 19:38:24] \"GET /parameters HTTP/1.1\" 200 -\n",
      "172.28.0.1 - - [12/Oct/2022 19:38:50] \"POST /update HTTP/1.1\" 200 -\n",
      "172.28.0.1 - - [12/Oct/2022 19:38:50] \"GET /parameters HTTP/1.1\" 200 -\n",
      "172.28.0.1 - - [12/Oct/2022 19:38:59] \"POST /update HTTP/1.1\" 200 -\n",
      "172.28.0.1 - - [12/Oct/2022 19:38:59] \"GET /parameters HTTP/1.1\" 200 -\n",
      "172.28.0.1 - - [12/Oct/2022 19:41:55] \"POST /update HTTP/1.1\" 200 -\n",
      "172.28.0.1 - - [12/Oct/2022 19:42:13] \"POST /update HTTP/1.1\" 200 -\n",
      "172.28.0.1 - - [12/Oct/2022 19:42:16] \"POST /update HTTP/1.1\" 200 -\n",
      "172.28.0.1 - - [12/Oct/2022 19:42:16] \"GET /parameters HTTP/1.1\" 200 -\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      ">>> Async training complete.\n",
      "2416.637028694153\n"
     ]
    }
   ],
   "source": [
    "import time\n",
    "t1 = time.time()\n",
    "my_dl1 = dl_pipeline.fit(trainqq1) # cnn 5th not finetuning\n",
    "t3 = time.time() - t1\n",
    "print(t3)\n",
    "aaaaaa = \"완료!!\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "64cce30d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ElephasTransformer_6abbb57d10d0\n"
     ]
    }
   ],
   "source": [
    "cs_model = my_dl1.stages[0]\n",
    "print(cs_model)\n",
    "\n",
    "cs_model.save(\"/root/spark/bigcomp_model/model/typoon/cnn/fine5\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "0ba570a4",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "root\n",
      " |-- label_index: double (nullable = true)\n",
      " |-- prediction2: double (nullable = true)\n",
      "\n"
     ]
    }
   ],
   "source": [
    "\n",
    "\n",
    "from pyspark.ml.evaluation import MulticlassClassificationEvaluator\n",
    "from pyspark.mllib.evaluation import MulticlassMetrics\n",
    "from pyspark.ml.evaluation import BinaryClassificationEvaluator\n",
    "from pyspark.mllib.evaluation import BinaryClassificationMetrics\n",
    "\n",
    "#pred_test = my_dl.transform(label_dft)\n",
    "pred_test = my_dl.transform(testqq)\n",
    "from pyspark.sql.functions import udf\n",
    "list_to_double = udf(lambda l: l[0], DoubleType())\n",
    "pred_test = pred_test.select(\n",
    "    pred_test[\"label_index\"], \n",
    "    list_to_double(pred_test[\"prediction\"]).alias(\"prediction2\")\n",
    ")\n",
    "pred_test.printSchema()\n",
    "\n",
    "from pyspark.sql.functions import udf\n",
    "list_to_vector_udf = udf(lambda l:1.0 if l<0.5 else 0.0 , DoubleType())\n",
    "pred_test = pred_test.select(\n",
    "    pred_test[\"label_index\"],\n",
    "    pred_test[\"prediction2\"],\n",
    "    list_to_vector_udf(pred_test[\"prediction2\"]).alias(\"rawPrediction\")\n",
    ")\n",
    "import pandas as pd\n",
    "from sklearn.datasets import make_classification\n",
    "from sklearn.metrics import accuracy_score, f1_score, precision_score, recall_score, classification_report, confusion_matrix\n",
    "dfdf1 = pred_test.toPandas() # fine-tuned RNN 2nd window\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "id": "91a52e65",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      ">>> Fit model\n",
      ">>> Initialize workers\n",
      ">>> Distribute load\n",
      " * Serving Flask app 'elephas.parameter.server' (lazy loading)\n",
      " * Environment: production\n",
      "\u001b[31m   WARNING: This is a development server. Do not use it in a production deployment.\u001b[0m\n",
      "\u001b[2m   Use a production WSGI server instead.\u001b[0m\n",
      " * Debug mode: off\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:werkzeug: * Running on http://172.28.0.1:4000/ (Press CTRL+C to quit)\n",
      "INFO:werkzeug:172.28.0.2 - - [05/Oct/2022 08:17:59] \"GET /parameters HTTP/1.1\" 200 -\n",
      "INFO:werkzeug:172.28.0.1 - - [05/Oct/2022 08:18:02] \"GET /parameters HTTP/1.1\" 200 -\n",
      "INFO:werkzeug:172.28.0.3 - - [05/Oct/2022 08:18:03] \"GET /parameters HTTP/1.1\" 200 -\n",
      "INFO:werkzeug:172.28.0.2 - - [05/Oct/2022 08:19:03] \"GET /parameters HTTP/1.1\" 200 -\n",
      "INFO:werkzeug:172.28.0.1 - - [05/Oct/2022 08:19:12] \"GET /parameters HTTP/1.1\" 200 -\n",
      "INFO:werkzeug:172.28.0.3 - - [05/Oct/2022 08:19:24] \"GET /parameters HTTP/1.1\" 200 -\n",
      "INFO:werkzeug:172.28.0.1 - - [05/Oct/2022 08:19:46] \"GET /parameters HTTP/1.1\" 200 -\n",
      "INFO:werkzeug:172.28.0.1 - - [05/Oct/2022 08:36:49] \"POST /update HTTP/1.1\" 200 -\n",
      "INFO:werkzeug:172.28.0.1 - - [05/Oct/2022 08:36:50] \"GET /parameters HTTP/1.1\" 200 -\n",
      "INFO:werkzeug:172.28.0.1 - - [05/Oct/2022 08:40:21] \"POST /update HTTP/1.1\" 200 -\n",
      "INFO:werkzeug:172.28.0.1 - - [05/Oct/2022 08:40:21] \"GET /parameters HTTP/1.1\" 200 -\n",
      "INFO:werkzeug:172.28.0.1 - - [05/Oct/2022 08:41:31] \"POST /update HTTP/1.1\" 200 -\n",
      "INFO:werkzeug:172.28.0.1 - - [05/Oct/2022 08:41:31] \"GET /parameters HTTP/1.1\" 200 -\n",
      "INFO:werkzeug:172.28.0.1 - - [05/Oct/2022 08:58:14] \"POST /update HTTP/1.1\" 200 -\n",
      "INFO:werkzeug:172.28.0.1 - - [05/Oct/2022 08:58:14] \"GET /parameters HTTP/1.1\" 200 -\n",
      "INFO:werkzeug:172.28.0.1 - - [05/Oct/2022 09:01:53] \"POST /update HTTP/1.1\" 200 -\n",
      "INFO:werkzeug:172.28.0.1 - - [05/Oct/2022 09:01:53] \"GET /parameters HTTP/1.1\" 200 -\n",
      "INFO:werkzeug:172.28.0.1 - - [05/Oct/2022 09:02:56] \"POST /update HTTP/1.1\" 200 -\n",
      "INFO:werkzeug:172.28.0.1 - - [05/Oct/2022 09:02:56] \"GET /parameters HTTP/1.1\" 200 -\n",
      "INFO:werkzeug:172.28.0.1 - - [05/Oct/2022 09:19:05] \"POST /update HTTP/1.1\" 200 -\n",
      "INFO:werkzeug:172.28.0.1 - - [05/Oct/2022 09:19:05] \"GET /parameters HTTP/1.1\" 200 -\n",
      "INFO:werkzeug:172.28.0.1 - - [05/Oct/2022 09:22:51] \"POST /update HTTP/1.1\" 200 -\n",
      "INFO:werkzeug:172.28.0.1 - - [05/Oct/2022 09:22:51] \"GET /parameters HTTP/1.1\" 200 -\n",
      "INFO:werkzeug:172.28.0.1 - - [05/Oct/2022 09:23:59] \"POST /update HTTP/1.1\" 200 -\n",
      "INFO:werkzeug:172.28.0.1 - - [05/Oct/2022 09:23:59] \"GET /parameters HTTP/1.1\" 200 -\n",
      "INFO:werkzeug:172.28.0.1 - - [05/Oct/2022 09:39:55] \"POST /update HTTP/1.1\" 200 -\n",
      "INFO:werkzeug:172.28.0.1 - - [05/Oct/2022 09:39:55] \"GET /parameters HTTP/1.1\" 200 -\n",
      "INFO:werkzeug:172.28.0.1 - - [05/Oct/2022 09:43:41] \"POST /update HTTP/1.1\" 200 -\n",
      "INFO:werkzeug:172.28.0.1 - - [05/Oct/2022 09:43:41] \"GET /parameters HTTP/1.1\" 200 -\n",
      "INFO:werkzeug:172.28.0.1 - - [05/Oct/2022 09:45:07] \"POST /update HTTP/1.1\" 200 -\n",
      "INFO:werkzeug:172.28.0.1 - - [05/Oct/2022 09:45:07] \"GET /parameters HTTP/1.1\" 200 -\n",
      "INFO:werkzeug:172.28.0.1 - - [05/Oct/2022 10:00:53] \"POST /update HTTP/1.1\" 200 -\n",
      "INFO:werkzeug:172.28.0.1 - - [05/Oct/2022 10:00:53] \"GET /parameters HTTP/1.1\" 200 -\n",
      "INFO:werkzeug:172.28.0.1 - - [05/Oct/2022 10:04:39] \"POST /update HTTP/1.1\" 200 -\n",
      "INFO:werkzeug:172.28.0.1 - - [05/Oct/2022 10:04:39] \"GET /parameters HTTP/1.1\" 200 -\n",
      "INFO:werkzeug:172.28.0.1 - - [05/Oct/2022 10:06:05] \"POST /update HTTP/1.1\" 200 -\n",
      "INFO:werkzeug:172.28.0.1 - - [05/Oct/2022 10:06:05] \"GET /parameters HTTP/1.1\" 200 -\n",
      "INFO:werkzeug:172.28.0.1 - - [05/Oct/2022 10:21:39] \"POST /update HTTP/1.1\" 200 -\n",
      "INFO:werkzeug:172.28.0.1 - - [05/Oct/2022 10:21:39] \"GET /parameters HTTP/1.1\" 200 -\n",
      "INFO:werkzeug:172.28.0.1 - - [05/Oct/2022 10:25:34] \"POST /update HTTP/1.1\" 200 -\n",
      "INFO:werkzeug:172.28.0.1 - - [05/Oct/2022 10:25:34] \"GET /parameters HTTP/1.1\" 200 -\n",
      "INFO:werkzeug:172.28.0.1 - - [05/Oct/2022 10:27:14] \"POST /update HTTP/1.1\" 200 -\n",
      "INFO:werkzeug:172.28.0.1 - - [05/Oct/2022 10:27:14] \"GET /parameters HTTP/1.1\" 200 -\n",
      "INFO:werkzeug:172.28.0.1 - - [05/Oct/2022 10:42:31] \"POST /update HTTP/1.1\" 200 -\n",
      "INFO:werkzeug:172.28.0.1 - - [05/Oct/2022 10:42:31] \"GET /parameters HTTP/1.1\" 200 -\n",
      "INFO:werkzeug:172.28.0.1 - - [05/Oct/2022 10:46:23] \"POST /update HTTP/1.1\" 200 -\n",
      "INFO:werkzeug:172.28.0.1 - - [05/Oct/2022 10:46:23] \"GET /parameters HTTP/1.1\" 200 -\n",
      "INFO:werkzeug:172.28.0.1 - - [05/Oct/2022 10:48:19] \"POST /update HTTP/1.1\" 200 -\n",
      "INFO:werkzeug:172.28.0.1 - - [05/Oct/2022 10:48:19] \"GET /parameters HTTP/1.1\" 200 -\n",
      "INFO:werkzeug:172.28.0.1 - - [05/Oct/2022 11:03:16] \"POST /update HTTP/1.1\" 200 -\n",
      "INFO:werkzeug:172.28.0.1 - - [05/Oct/2022 11:03:16] \"GET /parameters HTTP/1.1\" 200 -\n",
      "INFO:werkzeug:172.28.0.1 - - [05/Oct/2022 11:07:06] \"POST /update HTTP/1.1\" 200 -\n",
      "INFO:werkzeug:172.28.0.1 - - [05/Oct/2022 11:07:06] \"GET /parameters HTTP/1.1\" 200 -\n",
      "INFO:werkzeug:172.28.0.1 - - [05/Oct/2022 11:09:11] \"POST /update HTTP/1.1\" 200 -\n",
      "INFO:werkzeug:172.28.0.1 - - [05/Oct/2022 11:09:11] \"GET /parameters HTTP/1.1\" 200 -\n",
      "INFO:werkzeug:172.28.0.1 - - [05/Oct/2022 11:23:58] \"POST /update HTTP/1.1\" 200 -\n",
      "INFO:werkzeug:172.28.0.1 - - [05/Oct/2022 11:23:58] \"GET /parameters HTTP/1.1\" 200 -\n",
      "INFO:werkzeug:172.28.0.1 - - [05/Oct/2022 11:27:47] \"POST /update HTTP/1.1\" 200 -\n",
      "INFO:werkzeug:172.28.0.1 - - [05/Oct/2022 11:27:47] \"GET /parameters HTTP/1.1\" 200 -\n",
      "INFO:werkzeug:172.28.0.1 - - [05/Oct/2022 11:30:02] \"POST /update HTTP/1.1\" 200 -\n",
      "INFO:werkzeug:172.28.0.1 - - [05/Oct/2022 11:30:02] \"GET /parameters HTTP/1.1\" 200 -\n",
      "INFO:werkzeug:172.28.0.1 - - [05/Oct/2022 11:44:41] \"POST /update HTTP/1.1\" 200 -\n",
      "INFO:werkzeug:172.28.0.1 - - [05/Oct/2022 11:47:03] \"POST /update HTTP/1.1\" 200 -\n",
      "INFO:werkzeug:172.28.0.1 - - [05/Oct/2022 11:47:46] \"POST /update HTTP/1.1\" 200 -\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      ">>> Async training complete.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:werkzeug:172.28.0.1 - - [05/Oct/2022 11:47:46] \"GET /parameters HTTP/1.1\" 200 -\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "14590.308893918991\n"
     ]
    }
   ],
   "source": [
    "import time\n",
    "t1 = time.time()\n",
    "my_dl = dl_pipeline.fit(trainqq1)\n",
    "t3 = time.time() - t1\n",
    "print(t3)\n",
    "aaaaaa = \"완료!!\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "768a25c2",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c4e7534d",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "id": "e9a37a19",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ElephasTransformer_c1d598385fd1\n"
     ]
    }
   ],
   "source": [
    "cs_model = my_dl.stages[0]\n",
    "print(cs_model)\n",
    "\n",
    "cs_model.save(\"/root/spark/bigcomp_model/model/typoon/lstm/fifth\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "id": "845e5688",
   "metadata": {},
   "outputs": [],
   "source": [
    "pred_test = my_dl.transform(testqq1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "976d490d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-----------+--------------------+--------------------+-----+----------+\n",
      "|label_index|            feature1|            feature2|label|prediction|\n",
      "+-----------+--------------------+--------------------+-----+----------+\n",
      "|        1.0|[1.0,0.0,0.0,0.0,...|[0.0,0.0,0.0,0.0,...|    1|[0.0, 1.0]|\n",
      "|        1.0|[1.0,0.0,0.0,0.0,...|[4001.0,4586.0,0....|    1|[0.0, 1.0]|\n",
      "|        1.0|[1.0,10.0,1.0,177...|[333.0,141.0,65.0...|    1|[0.0, 1.0]|\n",
      "|        1.0|[1.0,10.0,160.0,1...|[333.0,141.0,65.0...|    1|[0.0, 1.0]|\n",
      "|        1.0|[1.0,10.0,1920.0,...|[333.0,2129.0,508...|    1|[0.0, 1.0]|\n",
      "+-----------+--------------------+--------------------+-----+----------+\n",
      "only showing top 5 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "pred_test.show(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "id": "2d2cf9df",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "타이푼 finetuning second :  2257.6532146930695\n"
     ]
    }
   ],
   "source": [
    "print(\"타이푼 finetuning second : \",t3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "id": "3d340249",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "타이푼 third :  7631.656853199005\n"
     ]
    }
   ],
   "source": [
    "print(\"타이푼 third : \",t3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "id": "827e50d7",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "타이푼 fourth :  10176.846570968628\n"
     ]
    }
   ],
   "source": [
    "print(\"타이푼 fourth : \",t3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2fca2fc5",
   "metadata": {},
   "outputs": [],
   "source": [
    "trainqq.toPandas().to_csv(\"/root/spark/bigcomp_model/typoon/third_tr.csv\",index=False)\n",
    "testqq.toPandas().to_csv(\"/root/spark/bigcomp_model/typoon/third_ts.csv\",index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "id": "7d7745d4",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "root\n",
      " |-- label_index: double (nullable = false)\n",
      " |-- prediction2: double (nullable = true)\n",
      "\n"
     ]
    }
   ],
   "source": [
    "\n",
    "\n",
    "from pyspark.ml.evaluation import MulticlassClassificationEvaluator\n",
    "from pyspark.mllib.evaluation import MulticlassMetrics\n",
    "from pyspark.ml.evaluation import BinaryClassificationEvaluator\n",
    "from pyspark.mllib.evaluation import BinaryClassificationMetrics\n",
    "\n",
    "#pred_test = my_dl.transform(label_dft)\n",
    "pred_test = my_dl.transform(testqq1)\n",
    "from pyspark.sql.functions import udf\n",
    "list_to_double = udf(lambda l: l[0], DoubleType())\n",
    "pred_test = pred_test.select(\n",
    "    pred_test[\"label_index\"], \n",
    "    list_to_double(pred_test[\"prediction\"]).alias(\"prediction2\")\n",
    ")\n",
    "pred_test.printSchema()\n",
    "\n",
    "from pyspark.sql.functions import udf\n",
    "list_to_vector_udf = udf(lambda l:1.0 if l<0.5 else 0.0 , DoubleType())\n",
    "pred_test = pred_test.select(\n",
    "    pred_test[\"label_index\"],\n",
    "    pred_test[\"prediction2\"],\n",
    "    list_to_vector_udf(pred_test[\"prediction2\"]).alias(\"rawPrediction\")\n",
    ")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "efe95645",
   "metadata": {},
   "outputs": [],
   "source": [
    "pred_test.show(50)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "da75050d",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
